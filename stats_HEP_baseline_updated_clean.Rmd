---
title: "HEP_baseline_analysis"
output:
  pdf_document: default
  html_document: default
date: "2024-08-22"
editor_options: 
  chunk_output_type: inline
---

# A. Variables Overview: Bold =\> needs to be imported from other DFs

Missing variables are *marked*

1)  Self-report/Behavioral Variables:

-   Control Variables: age, sex, BMI, smoker, education_years, caffeine, heart rate knowledge, chlorpromazine equivalent dose
-   Additional Control Variables: other medications, handedness, HCT strategy
-   Interoceptive Sensibility (= self-reported interoception) The Multidimensional Assessment of Interoceptive Awareness (MAIA), Body Perception Questionnaire (BPQ))
-   Interoceptive Accuracy\
    IAcc: mean percentage correctly perceived heartbeats)
-   Bodily Self-Consciousness Alterations/Depersonalization: Cambridge Depersonalization Scale (CDS)

2)  Neurophysiological Measures

-   Neural Correlates of Interception: HEP, mean amplitude 450-500 msec post- ECG R-peak
-   ECG parameters: HR, HRV (Root Mean Square of Successive Differences, RMSSD), QT interval, QTc interval, Amplitude of R wave
-   BMI (used as a control variable)

3)  *Clinical variables:*

PATIENTS - Symptoms: Severity: PANSS positive, PANSS negative, PANSS general psychopathology, PANSS total - Cognitive Impairments: BACS composite - General functioning: FROGS total

HCS - Potential psychopathology: Brief Psychiatric Rating Scale (BPRS)

NOTE: Patients' Data I need from other CSVs: BMI, education_years =\> these I added to the csv clozapine dose, chlorpromazine equivalent dose, PANSS, BACS, FROGS =\> these I will later retrieve for the correlation analyses

# B. Data Handling: General Steps

Goal: make HC and SZ data compatible, Merge Datasets, make df ready to analyse

-   Task 1: Import packages and data. Rename columns consistently.

Import libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)    # For reading CSV files
library(dplyr)    # For data manipulation
library(ggplot2)  # For plotting
library(ppcor)    # For partial correlation
library("psych")  # For partial correlation
library(VIM)      # For KNN imputation
library(caret)    # For dummy variables 
library(BayesFactor)
library(correlation)
library(see)
library(lme4)
library(emmeans)
library(apaTables)
library(psych)  
library(skimr)
library(rstatix)
library(see)
library(patchwork)
library(gtsummary)
library(tidyr)
library(gridExtra)
library(patchwork)
library(cowplot)
library(RColorBrewer)
library(lsr)
library(emmeans)
library(hms)
library(car)
library(lmtest)
library(readxl)
library(effectsize)
library("brms")
library(FactoMineR)
library(purrr)
library(mixOmics)





options(scipen=999)

knitr::opts_knit$set(root.dir = "/Users/denizyilmaz/Desktop/BrainTrain")

```

Import all data and tweak the columns to make them consistent as well as removing V3 and empty rows

```{r data import & skim, include=FALSE}

### 1. Import Behavioral Data
beh_data_hc <- read_xlsx('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/beh_data/BHC Healthy Controls Behavioral EEG_2025-09-04.xlsx')
# first row is descriptions so make colnames the second row
beh_data_hc$group <- "HC"
beh_data_hc <- beh_data_hc %>%
  rename(education_years = "education_years(dont count breaks)", CDS = `CDS (SUM ALL excluding 30)`)
# create BMI column for consistency btw. groups 
beh_data_hc$height <- beh_data_hc$height / 100  # Convert height from cm to m
beh_data_hc$body_mass_index <- beh_data_hc$weight / (beh_data_hc$height^2) # Calculate BMI
#make colname appropriate
colnames(beh_data_hc)[colnames(beh_data_hc) == "inclusion_criteria(1:fits,0:not)"] <- "inclusion_criteria"

# import sz data
beh_data_sz <- read_xlsx('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/beh_data/Patients EEG Behavioral Data & Preprocessing Log & Data Checklist_2025-09-04.xlsx')
# first row is descriptions so make colnames the second row
colnames(beh_data_sz) <- beh_data_sz[1, ] # Assign the first row as column names
beh_data_sz <- beh_data_sz[-1, ] # Remove the first row (now used as column names)
beh_data_sz$group <- "SSD"
# make the colnames equal
beh_data_sz <- beh_data_sz %>%
  rename(sex = Gender, age = Age)
# keep only BL for SZ
beh_data_sz_bl <- beh_data_sz %>%
  rename(session = Session) %>%  # Rename the column to lowercase 'session'
  filter(session == "V1")   # Keep only rows where session equals "V1"  

# Add this after importing beh_data_sz_bl
beh_data_sz_bl <- beh_data_sz_bl %>%
  mutate(group = ifelse(group == "SZ", "SSD", group))  # Convert SZ to SSD

### 2. Import clinical & cognitive data
clinical_data <- read_tsv("/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/clinical_data/2025-10-18_11_10_33_Klinische_komplett_PSY_BT_Klinische_TSV.tsv")
#make compatible
clinical_data_V1 <- clinical_data %>%
  rename(session = VISIT) %>%  # Rename the column to lowercase 'session'
  filter(session == "V1")  %>%      # Keep only rows where session equals "V1"  
  rename(subject = Internal_MEMBER_ID)


cog_data <- read_tsv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/clinical_data/2025-10-18_11_09_51_Klinische_komplett_PSY_BT_Kognition_TSV.tsv')
# Assign the first row as column names
#make compatible
cog_data_V1 <- cog_data %>%
  rename(session = VISIT) %>%  # Rename the column to lowercase 'session'
  filter(session == "V1")  %>%      # Keep only rows where session equals "V1"  
  rename(subject = Internal_MEMBER_ID) 

# import med data
med_data <- read_csv("/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/clinical_data/2025-10-26_11_52_32_Back_Up_complete_Dataset_1g__Medikamentenanamnese_CSV_.csv")
med_data_v0 <- med_data %>%
  rename(session = VISIT) %>%  # Rename the column to lowercase 'session'
  rename(cpz = PSY_BT_CPZ_Equivalent_CM_2) %>% 
  filter(session == "V0") %>% 
  rename(subject = Internal_MEMBER_ID) %>% 
  mutate(cpz = as.numeric(cpz)) %>% 
  select(subject, cpz)
  
### 3. Import the HEP data
hep_data_hc <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/HEP_BHC/HC_hep_outputs_20250920.csv')
hep_data_hc$group <- "HC"
hep_data_sz <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/HEP_BTSCZ/hep_outputs_20250920.csv')
hep_data_sz$group <- "SSD"
# make sub names compatible
hep_data_sz$subject_id <- gsub("^BTSCZ", "BT", hep_data_sz$subject_id )
# keep only BL for SZ
hep_data_sz_bl <- hep_data_sz[hep_data_sz$session == "V1",]


### 4. Import ECG data
ecg_data_hc <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/ECG_BHC/HC_ecg_outputs_20251008.csv')
ecg_data_hc$group <- "HC"
ecg_data_sz <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/ECG_BTSCZ/SSD_ecg_outputs_20251008.csv')
ecg_data_sz$group <- "SSD"
# make subject_id compatible with the other data!
ecg_data_sz$subject_id <- gsub("^BTSCZ", "BT", ecg_data_sz$subject_id)
# keep only BL for SZ
ecg_data_sz_bl <- ecg_data_sz[ecg_data_sz$session == "V1",]



```

-   Task 2: Merge all data

```{r}

### 1. Merge Beh Data

# to verify that we did not lose columns after merging
cols_in_hc_not_sz <- setdiff(colnames(beh_data_hc), colnames(beh_data_sz_bl))  # Columns in df1 that are not in df2
cols_in_sz_not_hc <- setdiff(colnames(beh_data_sz_bl), colnames(beh_data_hc)) # Columns in df2 that are not in df1
print(cols_in_hc_not_sz)
print(cols_in_sz_not_hc)

# Concatenate the data frames vertically  
beh_data_sz_bl <- mutate_all(beh_data_sz_bl, as.character) # adjust for silly mismatches first !
beh_data_hc <- mutate_all(beh_data_hc, as.character)# adjust for silly mismatches first !
beh_data <-  bind_rows(beh_data_sz_bl, beh_data_hc)

### 2. Merge Physio Data

#merge
hep_data <- bind_rows(hep_data_sz_bl, hep_data_hc)
# make compatible
hep_data <- hep_data%>%   
  rename(subject = subject_id)

# merge
ecg_data_merged <- rbind(ecg_data_hc, ecg_data_sz_bl)
# make compatible
ecg_data_merged <- ecg_data_merged%>%
  rename(subject = subject_id)

# merge ecg and HEP data 
#common_cols_ecg_hep <- intersect(colnames(hep_data), colnames(ecg_data_merged))
common_cols_ecg_hep <- c("subject", "session", "task", "group")
hep_ecg_data <- left_join(hep_data, ecg_data_merged, by = common_cols_ecg_hep)  #full_join, but some extra rows were in ecg bc I forgot to exclude them on the script so we will do left join...


### 3. Merge all datasets 
common_cols_beh_hep <- intersect(colnames(hep_ecg_data), colnames(beh_data))
hep_ecg_beh_data <- merge(hep_ecg_data, beh_data, by = common_cols_beh_hep)


### 4. Clinical Data
cli_data <- full_join(clinical_data_V1, cog_data_V1, by = c("subject", "session")) %>%
  left_join(med_data_v0, by = c("subject"))

cli_beh_data_sz <- left_join(beh_data_sz_bl, cli_data, by = c("subject", "session"))


hep_ecg_data_sz <- hep_ecg_data %>%
  filter(group == "SSD")

cli_beh_hep_data_sz <- merge(hep_ecg_data_sz, cli_beh_data_sz, by = c("subject", "session"))


```

-   Task 3: Verify Column/Data type (format) incl. transformations to create Dummy Variables & assign ref levels
 NOTE: If a var is numeric you should first convert it to string to be able to convert it to category.????
```{r}

# define variable types 
# those which actually look like strings
categorical_vars_string <- c("session", "group", "task", "sex", "HCT_version", "handedness")
# those which look like numbers so should be first converted to numbers to not be treated as strings e.g. 1 =/ 1.00
categorical_vars_num <- c("smoker", "had_caffeine", "HCT_counted_body", "knows_heartrate",  "blood_withdrawn", "lactate_done_before_V1", "changed_meds", "inclusion_criteria")

numeric_vars <- c("total_heart_events_nr", "total_epochs_nr", "good_epoch_count", "percentage_dropped_epochs", "Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude", "Fp2_max_amplitude", "F4_max_amplitude", "F8_max_amplitude", "Fp2_min_amplitude", "F8_min_amplitude", "Fp2_max_latency", "F4_max_latency", "F8_max_latency", "Fp2_min_latency", "F4_min_latency", "F8_min_latency", "mean_HEP_accross_channels",  "threshold_percentage_bad_epochs", "F4_min_amplitude", "Fp1_mean_HEP", "Fp2_mean_HEP", "F7_mean_HEP", "F3_mean_HEP", "Fz_mean_HEP", "F4_mean_HEP", "F8_mean_HEP", "FC5_mean_HEP", "FC1_mean_HEP", "FC2_mean_HEP", "FC6_mean_HEP", "T7_mean_HEP", "C3_mean_HEP", "Cz_mean_HEP", "C4_mean_HEP", "T8_mean_HEP", "TP9_mean_HEP", "CP5_mean_HEP", "CP1_mean_HEP", "CP2_mean_HEP", "CP6_mean_HEP", "TP10_mean_HEP", "P7_mean_HEP", "P3_mean_HEP", "Pz_mean_HEP", "P4_mean_HEP", "P8_mean_HEP", "PO9_mean_HEP", "O1_mean_HEP", "O2_mean_HEP", "PO10_mean_HEP", "heart_rate_bpm", "hrv_rmssd_ms", "R_peak_amplitude_mV", "QT_interval_ms", "QTc_interval_ms",  "new_sampling_freq", "ecg_epochs_tmin", "ecg_epochs_tmax", "age", "body_mass_index", "education_years", "BPQ_body_awareness", "BPQ_supra_diaphragmatic", "BPQ_sub_diaphragmatic", "BPQ_autonomic", "BPQ_total", "CDS", "MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting", "MAIA_total", "HCT_observed_t1", "HCT_reported_t1", "HCT_observed_t2", "HCT_reported_t2", "HCT_observed_t3", "HCT_reported_t3", "HCT_observed_t4", "HCT_reported_t4", "HCT_observed_t5", "HCT_reported_t5", "HCT_observed_t6", "HCT_reported_t6", "HCT_observed_t7", "HCT_reported_t7", "HCT_observed_t8", "HCT_reported_t8", "HCT_observed_t9", "HCT_reported_t9",  "clozapine", "days_since_last_exc", "height", "weight", "BPRS (cutoff: 20)", "PANSS_pos_P1_delusions", "PANSS_pos_P2_formal_thought_disorder", "PANSS_pos_P3_hallucination", "PANSS_pos_P4_arousal", "PANSS_pos_P5_megalomania", "PANSS_pos_P6_distrust_paranoia", "PANSS_pos_P7_hostility", "PANSS_neg_N1_flat_affect", "PANSS_neg_N2_emotional_withdrawal", "PANSS_neg_N3_lacking_affective_rapport", "PANSS_neg_N4_social_passivity_and_apathy", "PANSS_neg_N5_difficulties_in_abstract_thinking", "PANSS_neg_N6_lack_of_spontaneity_and_fluency_of_language", "PANSS_neg_N7_stereotypical_thoughts", "PANSS_pos_total", "PANSS_neg_total", "PANSS_psychopat_G1_worries_about_health", "PANSS_psychopat_G2_fear", "PANSS_psychopat_G3_guilt", "PANSS_psychopat_G4_tension", "PANSS_psychopat_G5_mannerisms_and_unnatural_posture", "PANSS_psychopat_G6_depressive_symptoms", "PANSS_psychopat_G7_motor_slowdown", "PANSS_psychopat_G8_uncooperative_behavior", "PANSS_psychopat_G9_unusal_thoughts", "PANSS_psychopat_G10_disorientation", "PANSS_psychopat_G11_lack_of_attention", "PANSS_psychopat_G12_lack_of_judgement_and_insight", "PANSS_psychopat_G13_weakness_of_will", "PANSS_psychopat_G14_lack_of_impulse_control", "PANSS_psychopat_G15_self_centredness", "PANSS_psychopat_G16_active_social_avoidance_behavior", "PANSS_psychopat", "PANSS_total", "BNSS_anhedonia", "BNSS_reaction_stressful_event", "BNSS_social_withdrawal", "BNSS_avolition", "BNSS_flattened_affect", "BNSS_alogy", "CDSS", "CGI", "GAF", "SOFAS", "FROGS_daily_life", "FROGS_activities", "FROGS_relations", "FROGS_coping", "FROGS_health_treatment", "FROGS_total", "WHOQOL_1", "WHOQOL_2", "WHOQOL_4", "WHOQOL_5", "WHOQOL_6", "WHOQOL_7", "WHOQOL_3","excluded", "excluded_ec", "excluded_eo", "excluded_hct_eeg", "excluded_hct_beh", "excluded_questionnaires")
  
string_vars <- c("subject","event_type", "event_times","epoch_time_window", "baseline_correction", "hep_time_window", "channels", "hep_max_amplitudes", "hep_max_latencies", "hep_min_amplitudes", "hep_min_latencies", "hep_mean_amplitudes", "hep_amplitudes_sd", "start_time_of_analysis", "analysis_duration", "channel_interpolated_due_too-many-bad-epochs", "epoch_strategy", "baseline", "time_window", "ecg_mean_amplitude_time_window", "ecg_sd_amplitude_time_window", "Questionnaire Notes", "EEG_date", "EEG_Time_Start", "EEG_Time_End", "eyes-closed_bad_channels", "eyes-open_bad_channels", "hct_bads","HCT_strategy", "HCT_exc_effect", "interpolated_channels", "Notes", "bad_spans", "adverse_events", "highest_edu_level", "meds", "age_mean")
  

# Specify levels for categorical variables
factor_levels <- list(
  session = c("V1"), #V3. # NA
  group = c("HC", "SSD"),
  task = c("eyes-closed", "eyes-open", "hct"), 
  sex = c("f", "m"),
  HCT_version = c("A","B"), 
  smoker = c("0","1"), 
  had_caffeine = c("0","1"), 
  HCT_counted_body = c("0","1"), # NA
  knows_heartrate = c("0","1"),#NA 
  blood_withdrawn = c("0","1"), #NA 
  lactate_done_before_V1 = c("0","1"), # NA
  changed_meds = c("0","1"), 
  excluded = c("0","1"), 
  excluded_ec = c("0","1"), 
  excluded_eo = c("0","1"), 
  excluded_hct_eeg = c("0","1"), 
  excluded_hct_beh = c("0","1"), 
  excluded_questionnaires = c("0","1"),
  handedness = c("right","left", "ambidextrous"), 
  inclusion_criteria = c("0","1") 
)


# define function to convert all cols
convert_columns <- function(df, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels) {
  # Convert string variables to character type
  for (var in string_vars) {
    if (var %in% names(df)) {
      df[[var]] <- as.character(df[[var]])
    }
  }
  
  # Convert numeric variables to numeric type
  for (var in numeric_vars) {
    if (var %in% names(df)) {
      df[[var]] <- as.numeric(df[[var]])
    }
  }
  
  # Convert variables to factors and apply levels to categorical variables
  for (var in categorical_vars_string) {
    if (var %in% names(df)) {
      df[[var]] <- as.factor(as.character(df[[var]]))  
      df[[var]] <- factor(df[[var]], levels = factor_levels[[var]])

    }
  }
    # Convert variables to factors and apply levels to categorical variables
  for (var in categorical_vars_num) {
    if (var %in% names(df)) {
      df[[var]] <- as.factor(as.double(df[[var]]))  
      df[[var]] <- factor(df[[var]], levels = factor_levels[[var]])

    }
  }
  return(df)
}


# Apply the function to your data frames
beh_data <- convert_columns(beh_data, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels)
hep_ecg_data <- convert_columns(hep_ecg_data, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels)
hep_ecg_beh_data <- convert_columns(hep_ecg_beh_data, string_vars, numeric_vars,categorical_vars_string, categorical_vars_num, factor_levels)

cli_beh_data_sz <- convert_columns(cli_beh_data_sz, string_vars, numeric_vars,categorical_vars_string, categorical_vars_num, factor_levels)
cli_beh_hep_data_sz <- convert_columns(cli_beh_hep_data_sz, string_vars, numeric_vars,categorical_vars_string, categorical_vars_num, factor_levels)


# Check data types using str()
str(hep_ecg_data)
str(beh_data)
str(hep_ecg_beh_data)
str(cli_beh_hep_data_sz)
str(cli_beh_data_sz)


```

-   Task 4.a: Exclude Bad Subjects

    -   In general excluded =>
        -   if a sub is fully excluded excluded = 1

```{r}

####### FULLY EXCLUDED SUBJECTS, exclude from beh data #########

# Get the subjects where excluded is 1
excluded_subs <- beh_data$subject[beh_data$excluded == 1]
# Remove any leading or trailing whitespace and unnecessary quotes (if any)
excluded_subs_clean <- trimws(excluded_subs)
# omit na s if needed
# excluded_subs_clean <- na.omit(excluded_subs_clean)
# Print the cleaned list of excluded subjects
print(excluded_subs_clean)

# now clean the data 
beh_data <- beh_data[!(beh_data$subject %in% excluded_subs), ]
hep_ecg_data <- hep_ecg_data[!(hep_ecg_data$subject %in% excluded_subs), ]
hep_ecg_beh_data <- hep_ecg_beh_data[!(hep_ecg_beh_data$subject %in% excluded_subs), ]

cli_beh_hep_data_sz <- cli_beh_hep_data_sz[!(cli_beh_hep_data_sz$subject %in% excluded_subs), ]
cli_beh_data_sz <-  cli_beh_data_sz[!(cli_beh_data_sz$subject %in% excluded_subs), ]


```

-   Task 4.b: Create Indices (necessary variables that are derivatives of existing vars)

    -   IAcc: interoceptive_accuracy
    -   MAIA_total
    -   BACS Composite Cognition Scores calculation for SZ

```{r}


### Sum the specified MAIA columns to create MAIA_total
beh_data$MAIA_total <- rowSums(beh_data[, c("MAIA_noticing", "MAIA_not_distracting", 
                                    "MAIA_not_worrying", "MAIA_attention_regulation", 
                                    "MAIA_emotional_awareness", "MAIA_self_regulation", 
                                    "MAIA_body_listening", "MAIA_trusting")], na.rm = TRUE)

hep_ecg_beh_data$MAIA_total <- rowSums(hep_ecg_beh_data[, c("MAIA_noticing", "MAIA_not_distracting", 
                                    "MAIA_not_worrying", "MAIA_attention_regulation", 
                                    "MAIA_emotional_awareness", "MAIA_self_regulation", 
                                    "MAIA_body_listening", "MAIA_trusting")], na.rm = TRUE)
  

### IAcc
beh_data$interoceptive_accuracy <- numeric(length(beh_data$subject))
hep_ecg_beh_data$interoceptive_accuracy <- numeric(length(hep_ecg_beh_data$subject))


# Loop through each subject
for (i in 1:length(beh_data$subject)) {
  # Initialize a vector to store accuracies for each trial
  accuracies <- numeric(9)  # for the 9 trials
  
  # Loop through each trial (1 to 9)
  for (r in 1:9) {
    # Calculate the accuracy for the r-th trial
    observed_heartbeats <- beh_data[[paste0("HCT_observed_t", r)]][i]
    reported_heartbeats <- beh_data[[paste0("HCT_reported_t", r)]][i]
    
    # Calculate the accuracy for this trial
    accuracy <- 1 - abs(observed_heartbeats - reported_heartbeats) / observed_heartbeats
    
    # Store the accuracy
    accuracies[r] <- accuracy
  }
  
  # Calculate the IAcc score as the mean accuracy across all trials
  beh_data$interoceptive_accuracy[i] <- mean(accuracies, na.rm = TRUE)  # Use na.rm = TRUE to handle any missing data
}

for (i in 1:length(hep_ecg_beh_data$subject)) {
  # Initialize a vector to store accuracies for each trial
  accuracies <- numeric(9)  # for the 9 trials
  
  # Loop through each trial (1 to 9)
  for (r in 1:9) {
    # Calculate the accuracy for the r-th trial
    observed_heartbeats <- hep_ecg_beh_data[[paste0("HCT_observed_t", r)]][i]
    reported_heartbeats <- hep_ecg_beh_data[[paste0("HCT_reported_t", r)]][i]
    
    # Calculate the accuracy for this trial
    accuracy <- 1 - abs(observed_heartbeats - reported_heartbeats) / observed_heartbeats
    
    # Store the accuracy
    accuracies[r] <- accuracy
  }
  
  # Calculate the IAcc score as the mean accuracy across all trials
  hep_ecg_beh_data$interoceptive_accuracy[i] <- mean(accuracies, na.rm = TRUE)  # Use na.rm = TRUE to handle any missing data
}

### BACS composite & PANSS sums

# get PANSS scores as needed
cli_beh_data_sz <- cli_beh_data_sz %>%
  mutate(
    panss_pos = rowSums(select(., starts_with("PSY_BT_PANSS_P")), na.rm = TRUE),
    panss_neg = rowSums(select(., starts_with("PSY_BT_PANSS_N")), na.rm = TRUE),
    panss_gen = rowSums(select(., starts_with("PSY_BT_PANSS_G")), na.rm = TRUE),
    panss_total = panss_pos + panss_neg + panss_gen
  )

#BACS
# 1Ô∏è. Define raw BACS columns
bacs_items <- c(
  "PSY_BACS_vm_1", "PSY_BACS_vm_2", "PSY_BACS_vm_3", 
  "PSY_BACS_vm_4", "PSY_BACS_vm_5", "PSY_BACS_vm_6", "PSY_BACS_vm_7",
  "PSY_BACS_DIGITSEQUENCING",
  "PSY_BACS_TOKENMOTOR",
  "PSY_BACS_vf_animals", "PSY_BACS_vf_fwords", "PSY_BACS_vf_swords",
  "PSY_BACS_dsst_raw",
  "PSY_BACS_TOWEROFLONDON_VERSION_A"
)

cli_beh_data_sz <- cli_beh_data_sz %>%
  # 2. Convert all raw columns to numeric (fix char columns like _int)
  mutate(across(all_of(bacs_items), ~ as.numeric(as.character(.)), .names = "{.col}_num")) %>%
  
  # 3. Standardize each raw measure (z-score)
  mutate(across(ends_with("_num"), ~ as.numeric(scale(.)), .names = "{.col}_z")) %>%
  
  # 4. Construct-level scores (average of z-scored items)
  mutate(
    verbal_memory_z  = rowMeans(as.matrix(select(., matches("^PSY_BACS_vm_\\d+_num_z$"))), na.rm = TRUE),
    working_memory_z = PSY_BACS_DIGITSEQUENCING_num_z,
    motor_speed_z    = PSY_BACS_TOKENMOTOR_num_z,
    verbal_fluency_z = rowMeans(as.matrix(select(., matches("^PSY_BACS_vf_.*_num_z$"))), na.rm = TRUE),
    attention_z      = PSY_BACS_dsst_raw_num_z,
    executive_z      = PSY_BACS_TOWEROFLONDON_VERSION_A_num_z
  ) %>%
  
  # 5. Optional: Re-scale multi-item constructs to mean=0, SD=1
  mutate(
    verbal_memory_z  = as.numeric(scale(verbal_memory_z)),
    verbal_fluency_z = as.numeric(scale(verbal_fluency_z))
  ) %>%
  
  # 6. Compute global BACS composite (average of six constructs)
  mutate(
    BACS_composite_z = rowMeans(select(., verbal_memory_z, working_memory_z, motor_speed_z,
                                       verbal_fluency_z, attention_z, executive_z), na.rm = TRUE),
    # 7. Re-scale global composite
    BACS_composite_z = as.numeric(scale(BACS_composite_z))
  )

## Do the same for cli_beh_hep_data_sz
# get PANSS scores as needed
cli_beh_hep_data_sz <- cli_beh_hep_data_sz %>%
  mutate(
    panss_pos = rowSums(select(., starts_with("PSY_BT_PANSS_P")), na.rm = TRUE),
    panss_neg = rowSums(select(., starts_with("PSY_BT_PANSS_N")), na.rm = TRUE),
    panss_gen = rowSums(select(., starts_with("PSY_BT_PANSS_G")), na.rm = TRUE),
    panss_total = panss_pos + panss_neg + panss_gen
  )

#BACS
# 1Ô∏è. Define raw BACS columns
bacs_items <- c(
  "PSY_BACS_vm_1", "PSY_BACS_vm_2", "PSY_BACS_vm_3", 
  "PSY_BACS_vm_4", "PSY_BACS_vm_5", "PSY_BACS_vm_6", "PSY_BACS_vm_7",
  "PSY_BACS_DIGITSEQUENCING",
  "PSY_BACS_TOKENMOTOR",
  "PSY_BACS_vf_animals", "PSY_BACS_vf_fwords", "PSY_BACS_vf_swords",
  "PSY_BACS_dsst_raw",
  "PSY_BACS_TOWEROFLONDON_VERSION_A"
)

cli_beh_hep_data_sz <- cli_beh_hep_data_sz %>%
  # 2. Convert all raw columns to numeric (fix char columns like _int)
  mutate(across(all_of(bacs_items), ~ as.numeric(as.character(.)), .names = "{.col}_num")) %>%
  
  # 3. Standardize each raw measure (z-score)
  mutate(across(ends_with("_num"), ~ as.numeric(scale(.)), .names = "{.col}_z")) %>%
  
  # 4. Construct-level scores (average of z-scored items)
  mutate(
    verbal_memory_z  = rowMeans(as.matrix(select(., matches("^PSY_BACS_vm_\\d+_num_z$"))), na.rm = TRUE),
    working_memory_z = PSY_BACS_DIGITSEQUENCING_num_z,
    motor_speed_z    = PSY_BACS_TOKENMOTOR_num_z,
    verbal_fluency_z = rowMeans(as.matrix(select(., matches("^PSY_BACS_vf_.*_num_z$"))), na.rm = TRUE),
    attention_z      = PSY_BACS_dsst_raw_num_z,
    executive_z      = PSY_BACS_TOWEROFLONDON_VERSION_A_num_z
  ) %>%
  
  # 5. Optional: Re-scale multi-item constructs to mean=0, SD=1
  mutate(
    verbal_memory_z  = as.numeric(scale(verbal_memory_z)),
    verbal_fluency_z = as.numeric(scale(verbal_fluency_z))
  ) %>%
  
  # 6. Compute global BACS composite (average of six constructs)
  mutate(
    BACS_composite_z = rowMeans(select(., verbal_memory_z, working_memory_z, motor_speed_z,
                                       verbal_fluency_z, attention_z, executive_z), na.rm = TRUE),
    # 7. Re-scale global composite
    BACS_composite_z = as.numeric(scale(BACS_composite_z))
  )


  
```

-   Task 5: Missing Data: Impute behavioral vars with at least 90% data availability. Neurophysio. data won‚Äôt be imputed, if missing, will be discarded from the given analysis.

Package to visualize missing data: *mice*, provides methods to impute the data & analyse which vars are missing & are there corr.s !!!

```{r impute missing data }

missing_proportions <- cli_beh_data_sz %>%
   summarise(across(everything(), ~ mean(is.na(.)) * 100, .names = "{col}"))

# Convert missing_proportions to a tidy format
missing_proportions_long <- missing_proportions %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_proportion")

# Filter columns where missing_proportion is between 0 and 10
vars_to_impute <- missing_proportions_long %>%
  filter(missing_proportion > 0 & missing_proportion < 10) %>%
  pull(variable)

# View the result
print(vars_to_impute)
# do not impute these
no_impute_vars <- c("clozapine", "excluded_ec", "excluded_eo", "excluded_hct_eeg", "excluded_hct_beh", "excluded_questionnaires", "session", "task", "subject", "sex")

# Remove these variables from vars_to_impute
vars_to_impute <- setdiff(vars_to_impute, no_impute_vars)
# Print the updated list
print(vars_to_impute)


# Perform KNN imputation
if (length(vars_to_impute) == 0) {
    message("No variables with missing data to impute.")
} else {
    beh_data <- kNN(beh_data, k = 5, variable = vars_to_impute)
    colnames(beh_data) <- sub("\\.imp", "", colnames(beh_data)) # Remove the suffix added by kNN function (it adds .imp to imputed variables
    hep_ecg_beh_data <- kNN(hep_ecg_beh_data, k = 5, variable = vars_to_impute)
    colnames(hep_ecg_beh_data) <- sub("\\.imp", "", colnames(hep_ecg_beh_data)) # Remove the suffix added by kNN function (it adds .imp to imputed variables
    cli_beh_data_sz <- kNN(cli_beh_data_sz, k = 5, variable = vars_to_impute)
    colnames(cli_beh_data_sz) <- sub("\\.imp", "", colnames(cli_beh_data_sz))
    cli_beh_hep_data_sz <- kNN(cli_beh_hep_data_sz, k = 5, variable = vars_to_impute)
    colnames(cli_beh_hep_data_sz) <- sub("\\.imp", "", colnames(cli_beh_hep_data_sz))
}


```

-   Task 6: Create DFs per analysis by excluding subjects per ta

  -   For Questionnaires

        -   If excluded_questionnaires is 1, that row should be excluded

```{r}

questionnaire_data <- beh_data %>% # or with dplyr
  filter(excluded_questionnaires != 1 | is.na(excluded_questionnaires))

questionnaire_data %>% 
  count(group)

# save
write.csv(questionnaire_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/questionnaire_data.csv", row.names = FALSE)

```

  -   For IAcc

      -   If HCT_counted_only_body is 0 or excluded_hct_beh is 1 exclude those subs from HCT/IAcc analysis

```{r}

iacc_data <- beh_data %>% # or with dplyr
  filter(excluded_hct_beh != 1 | is.na(excluded_hct_beh)) %>% 
  filter(HCT_counted_body == 1)

# From the ECG data Get only HCT we need ecg parameters of that task
hep_ecg_data_hct <- hep_ecg_data %>%
  filter(task == "hct")

# merge ECG and beh_data on common. cols to create iacc_data
common_cols <- intersect(colnames(iacc_data), colnames(hep_ecg_data_hct))

iacc_data <- left_join(iacc_data, hep_ecg_data_hct, by = common_cols) %>% #  
  mutate(group = as.factor(group))

iacc_data %>% 
  count(group)

# save
write.csv(iacc_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/iacc_data.csv", row.names = FALSE)


```
 
  - For EEG analysis
  
        -   If excluded_ec/excluded_eo/excluded_hct corresponding column is 1, then corresponding run has already been excluded from the EEG data.
            -   DONE during preprocessing step 1.
            
        -   Data-quality related exclusion
        
            -   BAD EPOCHS: check % excluded epochs, if \> 33%, check whether if comes from one channel, if yes interpolate that channel, if not exclude person/task.
                -   DONE during preprocessing step 3.
                
            -   BAD CHANNELS:
                -   ROI analysis: if more than 1 of ROIs interpolated, exclude run
                -   Permutation Test: exclude people with more bod channels than 33%  (DONE in cluster_permutation_test.ipynb) EEG ROI


```{r}

# data containing all info for EEG analysis (HEPS, ECGs, behavioral&demographic): 
hep_ecg_beh_data


##### BAD CHANNELS #######

###  ROI DATA => if 2+ ROIs have been interpolated, remove subject from EEG analysis:

# get prep outputs
prep_output_sz <- read.csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/prep_outputs_for_analysis/SSD_prep_interp_output_sampling-250_bandpass-0.30-45.00_line-50_find_all_bads_20250821.csv')
prep_output_hc <- read.csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/prep_outputs_for_analysis/HC_prep_interp_output_sampling-250_bandpass-0.30-45.00_line-50_find_all_bads_20250819.csv') ## CHECK PATHS !!

# Perform a full outer join by column names
prep_output <- dplyr::bind_rows(prep_output_sz, prep_output_hc)

# make subject col compatible
prep_output <- prep_output %>%
  rename(subject = subject_id)

# make sub names compatible
prep_output$subject <- gsub("^BTSCZ", "BT", prep_output$subject )

# Keep rows where session is not "V3" and include rows with NA in session
prep_output <- prep_output %>%
  filter(session != "V3" | is.na(session))

# Convert `analysis_duration` in both data frames to <time>
hep_ecg_beh_data <- hep_ecg_beh_data %>%
  mutate(analysis_duration = hms::as_hms(analysis_duration))

# make compatib≈üe data types
prep_output <- prep_output %>%
  mutate(analysis_duration = hms::as_hms(analysis_duration))

# create a big dataset including prep, hep_ecg_beh_data
#common_cols_prep_hep <- intersect(colnames(hep_ecg_beh_data), colnames(prep_output))
common_cols_prep_hep <- c("subject", "session", "task") 
hep_ecg_beh_prep_data <- left_join(hep_ecg_beh_data, prep_output, by = common_cols_prep_hep) # was full_join but all data I actly need is on the left one..


# Define the channels to check
channels_to_check <- c("F4", "F8", "Fp2")

# Create the new dataframe
roi_hep_data <- hep_ecg_beh_prep_data %>%
  filter(
    rowSums(sapply(channels_to_check, function(chan) {
      grepl(chan, interpolated_chans)
    })) +
    rowSums(sapply(channels_to_check, function(chan) {
      grepl(chan, `channel_interpolated_due_too-many-bad-epochs`)
    })) <= 1
  )

roi_hep_data %>%
  filter(task == "eyes-closed") %>%  # Keep only rows where task is "eyes-closed"
  count(group)                       # Count sample sizes per group


#### WHO DID I EXCLUDE?

# Identify subject-task combinations in hep_ecg_beh_data but not in roi_hep_data
excluded_subject_tasks <- hep_ecg_beh_data %>%
  anti_join(roi_hep_data, by = c("subject", "task")) %>%
  select(subject, task) # Select only subject and task columns

# Combine subject-task exclusions with fully excluded subjects expanded to all tasks
final_roi_excluded_list <- excluded_subject_tasks %>%
  bind_rows(first_excluded_with_all_tasks) %>%
  distinct() # Remove duplicates if any
# View the final excluded list
print(final_roi_excluded_list)

# Save to CSV if needed
write.csv(excluded_subject_tasks, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/excluded_subject_tasks.csv", row.names = FALSE)
write.csv(hep_ecg_beh_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/hep_ecg_beh_data.csv", row.names = FALSE)


```


EEG Permutation

```{r}
### PERMUTATION TEST DATA =>  exclude people with more bod channels than 33%. ... CHECK LATER IF THAT IS GOOD !!!!

# Define the threshold for percent_interpolated_electrode
threshold <- 33

# Find rows where percent_interpolated_electrode is greater than the threshold
rows_to_exclude_permutation <- prep_output$percent_interpolated_electrode > threshold

# Filter rows in `prep_output` with 2 or more channels in `channels_to_check`
prep_output_filtered_permutation <- prep_output %>%
  rowwise() %>%
  filter(percent_interpolated_electrode >= 33)

# Select subject_id and task from the filtered prep_output
excluded_subjects_tasks_permutation <- prep_output_filtered_permutation %>%
  select(subject, task)

# Filter out rows in hep_ecg_beh_data that match subject_id and task from excluded_subjects_tasks
permutation_hep_data <- hep_ecg_beh_data %>%
  anti_join(excluded_subjects_tasks_permutation, by = c("subject", "task"))

# Check the new dataset
str(permutation_hep_data)
head(permutation_hep_data)

# save 
write.csv(permutation_hep_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/permutation_hep_data.csvR", row.names = FALSE)

```


-   Correlations

```{r}

# datasets 
#cli_beh_data_sz
#cli_beh_hep_data_sz

### 1.1. Corr of clinical & questionnaires
corr_questionnaire_clinical_data <- cli_beh_data_sz %>% # or with dplyr
  filter(excluded_questionnaires != 1 | is.na(excluded_questionnaires))

write.csv(corr_questionnaire_clinical_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/corr_questionnaire_clinical_data.csv", row.names = FALSE)

### 1.2. Corr of clinical & iacc
# From the ECG data Get only HCT we need ecg parameters of that task
hep_ecg_beh_data_hct_ssd_iacc <- hep_ecg_beh_data %>%
  filter(task == "hct") %>%
  filter(group=="SSD") %>%
  filter(excluded_hct_beh != 1 | is.na(excluded_hct_beh)) %>% 
  filter(HCT_counted_body == 1)


corr_iacc_clinical_data <- left_join(hep_ecg_beh_data_hct_ssd_iacc, cli_beh_data_sz, by = c("subject", "session", "sex", "age", "body_mass_index", "education_years"))

write.csv(corr_iacc_clinical_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/corr_iacc_clinical_data.csv", row.names = FALSE)

 
### 1.3. Corr of clinical & HEP ROI 

#cli_beh_hep_data_sz <- cli_beh_hep_data_sz %>%
#  mutate(ecg_mean_amplitude_time_window = as.numeric(ecg_mean_amplitude_time_window))


corr_clinical_hep_roi_data <- cli_beh_hep_data_sz

write.csv(corr_clinical_hep_roi_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/corr_clinical_hep_roi_data.csv", row.names = FALSE)



```

- Exploratory 

```{r PLS analysis}

pls_data <- corr_clinical_hep_roi_data

# calculate iacc
trial_indices <- 1:9

# Calculate accuracy for each trial
accuracy_mat <- sapply(trial_indices, function(r) {
  1 - abs(pls_data[[paste0("HCT_reported_t", r)]] - pls_data[[paste0("HCT_observed_t", r)]]) /
      pls_data[[paste0("HCT_observed_t", r)]]
})

# Take mean across trials for each subject
pls_data$interoceptive_accuracy <- rowMeans(accuracy_mat, na.rm = TRUE)

# calculate HEP as the most sig SSD amplitude..
pls_data$hep_pls <- rowMeans(pls_data[, c("CP1_mean_HEP", "CP2_mean_HEP")], na.rm = TRUE)

# Convert from volts to millivolts
pls_data$hep_pls <- pls_data$hep_pls * 1000

pls_cols <- c("subject", "task", "age", "sex", "body_mass_index", "cpz", "education_years",
              "panss_pos","panss_neg","panss_gen",
              "interoceptive_accuracy",
               "BPQ_body_awareness","BPQ_supra_diaphragmatic", "BPQ_sub_diaphragmatic", "CDS",
               "MAIA_noticing", "MAIA_body_listening", "MAIA_emotional_awareness", "MAIA_not_distracting", "MAIA_not_worrying",     "MAIA_attention_regulation", "MAIA_self_regulation", "MAIA_trusting",
              "hep_pls", "heart_rate_bpm", "hrv_rmssd_ms"
              )

# Keep only columns in pls_cols that exist in your dataframe
existing_cols <- intersect(pls_cols, colnames(pls_data))
pls_data <- pls_data[, existing_cols]

# Quick check
colnames(pls_data)

# now get the data averaged for all tasks 
pls_data <- pls_data %>%
  group_by(subject) %>%
  mutate(
    hep_pls_all_tasks = mean(hep_pls, na.rm = TRUE),
    heart_rate_bpm_all_tasks = mean(heart_rate_bpm, na.rm = TRUE),
    hrv_rmssd_ms_all_tasks = mean(hrv_rmssd_ms, na.rm = TRUE)
  ) %>%
  ungroup()

# Keep only EC rows
pls_data <- pls_data %>% filter(task == "eyes-closed")

# drop task col since it is no longer partitioned for tasks
pls_data <- pls_data %>% select(-task)
pls_data <- pls_data %>% select(-hep_pls)




```

-   Task 8: Inspect Variable Distributions to see if everything looks normal (any implausible data? outliers?) OR do before each analysis??!!

    -   Inspect Descriptives/Distributions

```{r}

#skim
skim(beh_data)

#descriptives
descriptive_table <- beh_data %>% 
  get_summary_stats()

# Summarize without NA values for mean, median, etc.
descriptive_table_no_na <- beh_data %>%
  summarise(across(where(is.numeric), list(mean = ~mean(. , na.rm = TRUE), 
                                           sd = ~sd(. , na.rm = TRUE),
                                     median = ~median(. , na.rm = TRUE))))

# skim by group
beh_data %>%
  dplyr::group_by(group) %>%
  skim()

group_counts <- table(beh_data$group)
print(group_counts)

# get group counts
beh_data %>%
  count(group)
beh_data %>%
  count(sex)


# Create density plot for each group
ggplot(data = beh_data, aes(x = education_years, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of education_years per Group",
       x = "education_years",
       y = "Density",
       color = "Group") +
  theme_minimal()

### inspect all variables

```

```{r def compare_groups fun}
compare_groups <- function(data, value_col, group_col) {
  # extract variable names
  value <- data[[value_col]]
  group <- data[[group_col]]
  
  # drop NAs
  df <- data.frame(value = value, group = group)
  df <- na.omit(df)
  
  # get unique groups automatically
  groups <- unique(df$group)
  if (length(groups) != 2) {
    stop("Grouping column must have exactly 2 groups.")
  }
  
  # descriptive stats
  desc <- aggregate(df$value, by = list(Group = df$group),
                    FUN = function(x) c(mean = mean(x), sd = sd(x), n = length(x)))
  desc <- do.call(data.frame, desc)
  colnames(desc) <- c("Group", "Mean", "SD", "N")
  
  # t-test
  ttest <- t.test(value ~ group, data = df, var.equal = FALSE)
  
  # print results
  cat("Descriptive statistics:\n")
  print(round(desc, 3))
  cat("\nüî¨ t-test Results:\n")
  cat(paste0("t(", round(ttest$parameter, 2), ") = ", 
             round(ttest$statistic, 3), 
             ",  p = ", signif(ttest$p.value, 4), "\n"))
  
  invisible(list(descriptives = desc, t_test = ttest))
}

```

```{r def & use compare_groups_multi_vars}
compare_groups_multi_vars <- function(data, value_cols, group_col) {
  
  # Check if value_cols is a vector of column names
  if (!is.vector(value_cols) || !all(value_cols %in% colnames(data))) {
    stop("value_cols must be a vector of column names present in data.")
  }
  
  # Check if group_col is a single column name present in data
  if (length(group_col) != 1 || !(group_col %in% colnames(data))) {
    stop("group_col must be a single column name present in data.")
  }
  
  # Get unique groups from the data
  groups <- unique(data[[group_col]])
  if (length(groups) != 2) {
    stop(paste0("Grouping column '", group_col, "' must have exactly 2 groups."))
  }
  
  # Helper function to perform comparison for a single value column
  perform_comparison <- function(value_col) {
    
    # Extract variable names
    value <- data[[value_col]]
    group <- data[[group_col]]
    
    # Drop NAs for this specific pair of columns
    df <- data.frame(value = value, group = group)
    df <- na.omit(df)
    
    # Descriptive stats
    desc <- aggregate(df$value, by = list(Group = df$group),
                      FUN = function(x) c(mean = mean(x), sd = sd(x), n = length(x)))
    
    # The structure of the aggregate output needs careful handling:
    desc <- do.call(data.frame, desc)
    colnames(desc) <- c("Group", "Mean", "SD", "N")
    
    # FIX: Only round the numeric columns (Mean, SD, N) 
    desc_to_print <- desc
    numeric_cols <- c("Mean", "SD", "N")
    if (all(numeric_cols %in% colnames(desc_to_print))) {
      desc_to_print[numeric_cols] <- round(desc_to_print[numeric_cols], 3)
    }
    
    # t-test (Welch's t-test by default, var.equal = FALSE)
    ttest <- t.test(value ~ group, data = df, var.equal = FALSE)
    
    # Print results for the current variable
    cat(paste0("========================================\n"))
    cat(paste0("Results for variable: ", value_col, "\n"))
    cat(paste0("========================================\n"))
    cat("Descriptive statistics:\n")
    print(desc_to_print) # Print the rounded version
    cat("\n t-test Results (Welch):\n")
    cat(paste0("t(", round(ttest$parameter, 2), ") = ", 
               round(ttest$statistic, 3), 
               ", p = ", signif(ttest$p.value, 4), "\n"))
    cat("----------------------------------------\n")
    
    # Return results as a list (using the original unrounded desc)
    return(list(variable = value_col, descriptives = desc, t_test = ttest))
  }
  
  # Apply the comparison function to all value columns
  results <- lapply(value_cols, perform_comparison)
  names(results) <- value_cols
  
  invisible(results)
}

# Define the name of your grouping column
group_col_name <- "group"

# Define numeric variables 
numeric_vars <- c( "education_years", "body_mass_index", "age",
  "BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", 
  "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic", 
  "MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", 
  "MAIA_attention_regulation", "MAIA_emotional_awareness", 
  "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting", 
  "MAIA_total", "CDS", "interoceptive_accuracy"
)
#  "Fp2_mean_amplitude","F4_mean_amplitude", "F8_mean_amplitude" ,  "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms",  "QTc_interval_ms", "R_peak_amplitude_mV", 

# 2. Call the function
results_all_cols <- compare_groups_multi_vars(
  data = beh_data, 
  value_cols = numeric_vars, 
  group_col = group_col_name
)
```

```{r categorical vars}

# Define categorical variables
cat_vars <- c("sex", "had_caffeine", "smoker", "knows_heartrate")

# Initialize empty list to store results
results <- list()

for (var in cat_vars) {
  # Create contingency table
  tab <- table(beh_data[[var]], beh_data$group)  # group = SSD vs HC
  
  # Get chi-square test
  chi <- chisq.test(tab)
  
  # Convert counts to percentages per group
  percent <- prop.table(tab, 2) * 100
  
  # Create a data frame for this variable
  df <- as.data.frame(tab)
  df$Percent <- round(as.vector(percent), 1)
  df$Chi2 <- round(chi$statistic, 2)
  df$p <- round(chi$p.value, 4)
  df$Variable <- var
  colnames(df) <- c("Level", "Group", "Count", "Percent", "Chi2", "p", "Variable")
  
  # Reorder columns
  df <- df[, c("Variable", "Level", "Group", "Count", "Percent", "Chi2", "p")]
  
  # Add to results
  results[[var]] <- df
}

# Combine all variables into one table
chi2_table <- bind_rows(results)
print(chi2_table)

write.csv(chi2_table, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/descriptives/chi2_descr_table.csv", row.names = FALSE)

```

```{r fun to inspect}

# Investigate ungrouped distribution of all vars in data
investigate_distribution_simple <- function(data, vars) {
  # Ensure vars exist in the dataset
  vars <- vars[vars %in% colnames(data)]
  
  if (length(vars) == 0) {
    stop("None of the specified variables are present in the dataset.")
  }
  
  # Generate density plots
  for (var in vars) {
    plot <- ggplot(data, aes(x = .data[[var]])) +
      geom_density(fill = "blue", alpha = 0.4) +
      labs(
        title = paste("Density Plot of", var),
        x = var,
        y = "Density"
      ) +
      theme_minimal()
    print(plot)
  }
}


beh_data %>%
  dplyr::group_by(group) %>%
  skim()


```

```{r fun to inspect by group }
# Investigate grouped distribution of all vars in data
investigate_distribution_grouped <- function(data, vars, group_var) {
  # Ensure vars and group_var exist in the dataset
  vars <- vars[vars %in% colnames(data)]
  if (!group_var %in% colnames(data)) {
    stop(paste("The specified group variable", group_var, "is not present in the dataset."))
  }
  if (length(vars) == 0) {
    stop("None of the specified variables are present in the dataset.")
  }
  
  # Generate density plots grouped by group_var
  for (var in vars) {
    plot <- ggplot(data, aes(x = .data[[var]], color = .data[[group_var]], group = .data[[group_var]])) +
      geom_density() +
      labs(
        title = paste("Density Plot of", var, "by", group_var),
        x = var,
        y = "Density",
        color = group_var
      ) +
      theme_minimal()
    print(plot)
    
    # Skim grouped summary
    summary_tbl <- data %>%
      group_by(.data[[group_var]]) %>%
      skimr::skim(.data[[var]])
    
    print(summary_tbl)
  }
}
```

```{r summarize fun}
summarize_data <- function(data, numeric_vars, group_vars) {
  
  # Filter variables to include only those that exist in the dataset
  numeric_vars <- numeric_vars[numeric_vars %in% colnames(data)]
  group_vars <- group_vars[group_vars %in% colnames(data)]

  # 1. Skim
  print(skim(data))
  
  # 2. Descriptive statistics
  descriptive_table <- data %>% 
    summarise(across(all_of(numeric_vars), list(mean = ~mean(. , na.rm = TRUE), 
                                                sd = ~sd(. , na.rm = TRUE),
                                                median = ~median(. , na.rm = TRUE))))
  print(descriptive_table)
  
  # 3. Grouped descriptives
  data %>%
    group_by(across(all_of(group_vars))) %>%
    skim() %>%
    print()
  
  # 4. Group counts
  group_counts <- data %>%
    count(across(all_of(group_vars))) 
  print(group_counts)
  
  # 5. Sex distribution by group
  if ("sex" %in% colnames(data)) {
    data %>%
      group_by(across(all_of(group_vars))) %>%
      count(sex) %>%
      mutate(proportion = n / sum(n)) %>%
      spread(sex, proportion, fill = 0) %>%
      print()
  }
  
  # 6. Density plots for all numeric variables
  for (var in numeric_vars) {
    ggplot(data, aes_string(x = var, color = group_vars, group = group_vars)) +
      geom_density() +
      labs(title = paste("Density Plot of", var, "per Group"),
           x = var,
           y = "Density",
           color = "Group") +
      theme_minimal() +
      print()
  }
}

```

```{r}

# Define numeric variables 
numeric_vars <- c(
  "BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", 
  "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic", 
  "MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", 
  "MAIA_attention_regulation", "MAIA_emotional_awareness", 
  "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting", 
  "MAIA_total", "CDS", "education_years", "body_mass_index", 
  "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", 
  "QTc_interval_ms", "R_peak_amplitude_mV", "interoceptive_accuracy",
  "Fp2_mean_amplitude","F4_mean_amplitude", "F8_mean_amplitude"
)
  # Define group variables
group_vars <- c("group", "task", "sex", "session", "smoker", "had_caffeine")

summarize_data(questionnaire_data, numeric_vars, group_vars)
summarize_data(iacc_data, numeric_vars, group_vars)
summarize_data(roi_hep_data, numeric_vars, group_vars)

```

-   Task 9: Check assumptions of the test you want to conduct!

CHECK ASSUMPTIONS OF EACH TEST

Assumptions for lm() (Linear Regression) 1. Linearity The relationship between predictors and the outcome variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The residuals have constant variance across all levels of the predictors. 4. Normality of Residuals The residuals are normally distributed. 5. No Multicollinearity Predictors are not highly correlated with each other. 6. No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

Assumptions for ANCOVA 1. Linearity The relationship between the covariate(s) and the dependent variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The variance of the dependent variable is the same across all groups. 4. Normality of Residuals The residuals are normally distributed within each group. 5. Homogeneity of Regression Slopes The relationship between the covariate(s) and the dependent variable is the same across groups. (This is specific to ANCOVA.) 6. No Multicollinearity (If Multiple Covariates) Covariates are not strongly correlated with each other. 7. No Outliers or High Leverage Points There are no extreme outliers or influential points that could distort the analysis.

Count GROUPS

```{r}
questionnaire_data |> group_by(group) |> pull(subject) |> unique()
```

# C. Hypothesis Testing

## 1) We hypothesize that individuals with SSD show altered interoception on three levels compared to healthy controls (HC):

### 1.1) Interoceptive sensibility differences measured by interoception questionnaires Multidimensional Assessment of Interoceptive Awareness (MAIA) and Body Perception Questionnaire (BPQ). The Cambridge Depersonalization Scale (CDS) will capture further differences in bodily self-consciousness. (Non-directional hypothesis)

We will build a regression model for each hypothesis, where the variables of interest (Interoceptive Sensibility and Interoceptive Accuracy data) will be the dependent variable and the diagnostic group (HC vs. SSD) will be the predictor, along with the covariates of age, gender, BMI, and years of education.

Define Questionnaires

```{r define questionnaire_vars and questionnaire_covars}
questionnaire_vars <- c("BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic","MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "MAIA_total", "CDS")

questionnaire_vars_totals <- c("BPQ_total", "MAIA_total", "CDS")

questionnaire_vars_no_totals <- c("BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic","MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation",         "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "CDS")

BPQ_vars <- c("BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic")
BPQ_vars_no_total <- c("BPQ_body_awareness", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic")


MAIA_vars <- c("MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "MAIA_total")
MAIA_vars_no_total <- c("MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting")


questionnaire_covars_numeric = c("age", "education_years", "body_mass_index")

questionnaire_covars_categorical = c("group", "sex")


```

INSPECT

```{r}
investigate_distribution_simple(data = questionnaire_data, vars = questionnaire_vars)
investigate_distribution_grouped(data = questionnaire_data, vars = questionnaire_vars, group_var = "group")
investigate_distribution_grouped(data = questionnaire_data, vars = questionnaire_covars_numeric, group_var = "group")

# also check freqs of categorical vars
questionnaire_data %>%
  group_by(group) %>%
  count(sex) %>%
  mutate(proportion = n / sum(n)) %>%
  spread(sex, proportion, fill = 0)


```

#### Check for Assumptions for lm() (Linear Regression)

1.  Linearity The relationship between predictors and the outcome variable is linear.
2.  Independence of Observations Observations are independent of each other.
3.  Homoscedasticity The residuals have constant variance across all levels of the predictors.
4.  Normality of Residuals The residuals are normally distributed.
5.  No Multicollinearity Predictors are not highly correlated with each other.
6.  No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

#### TESTS

assumptions fun with plots
```{r}
check_lm_assumptions <- function(model, data) {
  library(car)       # For VIF and crPlots
  library(lmtest)    # For bptest, resettest, and raintest
  library(nortest)   # For Shapiro-Wilk test

  results <- list()

  # Linearity
  message("Checking Linearity...")
  results$linearity <- tryCatch({
    crPlots(model)  # Component + Residual Plots
    raintest_result <- raintest(model)  # Rainbow test
    if (raintest_result$p.value > 0.05) {
      "Pass: No evidence of nonlinearity (Rainbow test p > 0.05)."
    } else {
      "Fail: Evidence of nonlinearity (Rainbow test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not generate linearity test or plots."
  })

  # Homoscedasticity
  message("Checking Homoscedasticity...")
  results$homoscedasticity <- tryCatch({
    bp <- bptest(model)
    if (bp$p.value > 0.05) {
      "Pass: Residuals are homoscedastic (Breusch-Pagan test p > 0.05)."
    } else {
      "Fail: Residuals show heteroscedasticity (Breusch-Pagan test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Breusch-Pagan test."
  })

  # Normality of Residuals
  message("Checking Normality of Residuals...")
  results$normality <- tryCatch({
    shapiro <- shapiro.test(residuals(model))
    if (shapiro$p.value > 0.05) {
      "Pass: Residuals are approximately normal (Shapiro-Wilk test p > 0.05)."
    } else {
      "Fail: Residuals deviate from normality (Shapiro-Wilk test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Shapiro-Wilk test."
  })

  # Multicollinearity
  message("Checking Multicollinearity...")
  results$multicollinearity <- tryCatch({
    vif_values <- vif(model)
    problematic <- names(vif_values[vif_values > 5])
    if (length(problematic) == 0) {
      "Pass: No problematic multicollinearity (all VIFs < 5)."
    } else {
      paste("Fail: Problematic multicollinearity detected for", paste(problematic, collapse = ", "), ".")
    }
  }, error = function(e) {
    "Could not compute VIF."
  })

  # Model Specification
  message("Checking Model Specification...")
  results$model_specification <- tryCatch({
    reset <- resettest(model)
    if (reset$p.value > 0.05) {
      "Pass: Model appears well-specified (RESET test p > 0.05)."
    } else {
      "Fail: Model may be misspecified (RESET test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform RESET test."
  })

  # Outliers
  message("Checking Outliers...")
  results$outliers <- tryCatch({
    outlier <- outlierTest(model)
    if (is.null(outlier)) {
      "Pass: No significant outliers detected (Bonferroni p > 0.05)."
    } else {
      "Fail: Significant outliers detected. Inspect Studentized residuals."
    }
  }, error = function(e) {
    "Could not perform outlier test."
  })

  # Diagnostic Plots
  message("Generating Diagnostic Plots...")
  tryCatch({
    par(mfrow = c(2, 2))
    plot(model, which = 1:4)  # Generates Residuals vs Fitted, QQ, Scale-Location, and Residuals vs Leverage plots
    par(mfrow = c(1, 1))
  }, error = function(e) {
    message("Could not generate diagnostic plots.")
  })

  
    message("Generating Linearity Plots...")
    tryCatch({
    crPlots(model)
    "Linearity plot displayed."
  }, error = function(e) {
    "Could not generate linearity plot. Check your predictors."
  })
  # Return all results
  return(results)
}

```

TEST All Main Hypotheses with assumption checks: correct p for 3 questionnaires
```{r}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires/"

# Initialize a list to store the results
combined_results <- list()
combined_emms <- list()

for (var in questionnaire_vars_totals) { # or questionnaire_vars
  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  # Identify rows used in the linear model
  used_rows <- !is.na(questionnaire_data$group) & 
                complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])
  
  # Create a new dataframe with residuals and corresponding groups
  residuals_data <- data.frame(
    group = questionnaire_data$group[used_rows],
    residuals = residuals(lm_result)
  )
  
  # Step 2: Create a boxplot of residuals by group => To understand how the unexplained variance looks, NOT for the paper! 
  p <- ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Boxplot without separate outlier points
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +  # Show individual data points
    labs(title = paste("Distribution of Residuals by Group for", var), 
         x = "Group", 
         y = "Residuals") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set3")  # Optional: better color scheme
  
  print(p)
  
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Residuals_by_Group_", var, ".png"),
         plot = p,
         width = 8, height = 6, dpi = 300)

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
  # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Get 95% CI
  ci_df <- as.data.frame(confint(lm_result))
  ci_df$predictor <- rownames(ci_df)
  rownames(ci_df) <- NULL
  colnames(ci_df) <- c("CI_lower", "CI_upper", "predictor")
  
  # Merge CI with coefficients
  coef_df <- merge(coef_df, ci_df, by = "predictor")
  
   # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(lm_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  #combined_table <- full_join(anova_table, coef_df, effect_size_df, by = c("outcome", "predictor"))
  combined_table <- full_join(anova_table, coef_df, by = c("outcome", "predictor")) %>%
  full_join(effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
  
  # get emmeans
  emm <- emmeans(lm_result, ~ group )
  pairwise_results <- pairs(emm)
  summary(emm)
  plot(emm)

  # Convert emmeans results to a data frame
  emm_df <- as.data.frame(emm)
  
  # Add predictor and outcome columns to the coefficients data frame
  emm_df$predictor <- rownames(emm_df)
  rownames(emm_df) <- NULL
  emm_df$outcome <- rep(var, times = nrow(emm_df))
  
  # Store the combined results in the list
  combined_emms[[var]] <- emm_df
  
}

# Combine all results into one dataframe
anova_results_df_questionnaires_total <- do.call(rbind, combined_results)
anova_results_emms_questionnaires_total <- do.call(rbind, combined_emms)


# Save the data frame as a CSV file to the directory
write.csv(anova_results_df_questionnaires_total, file = file.path(plot_path, "lm_results_questionnaires_2025-09-05.csv"), row.names = FALSE)
write.csv(anova_results_emms_questionnaires_total, file = file.path(plot_path, "lm_results_questionnaires_emmeans_2025-09-05.csv"),  row.names = FALSE)


# Filter results for group comparison
anova_results_df_questionnaires_total_groupSZ <- anova_results_df_questionnaires_total[
  anova_results_df_questionnaires_total$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_questionnaires_total_groupSZ$p_adjusted <- p.adjust(anova_results_df_questionnaires_total_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_questionnaires_total_groupSZ)

# Save the data frame as a CSV file to the directory
write.csv(anova_results_df_questionnaires_total_groupSZ, file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-09-05.csv"), row.names = FALSE)

```

#### PLOTS

```{r publication plot maia}
library(ggplot2)
library(ggdist)
library(ggsignif)


pal <- c("#7A9E3A", "#D2B100")

names(pal) <- levels(factor(questionnaire_data$group))

maia_rainplot <- ggplot(
  questionnaire_data,
  aes(x = group, y = MAIA_total, fill = group, color = group)
) +
  stat_halfeye(
    adjust = 0.8,
    width = 0.45,
    #justification = -0.10,
    point_colour = NA,
    alpha = 0.55,              # ‚Üë less transparent
    linewidth = 0.9            # ‚Üë darker density outline
  ) +
  geom_boxplot(
    width = 0.25,
    outlier.shape = NA,
    alpha = 0.35,
    linewidth = 0.7            # ‚Üë box outline strength
  ) +
  geom_point(
    position = position_jitter(width = 0.10, height = 0),
    alpha = 0.75,              # ‚Üë point visibility
    size = 1.4
  ) +
  labs(
    x = "Group",
    y = "MAIA Total"
  ) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "transparent", color = NA),
    plot.background  = element_rect(fill = "transparent", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    panel.grid = element_blank(),
    axis.title = element_text(size = 24),
    axis.text  = element_text(size = 24)
  )+ scale_y_continuous(limits = c(10, NA),
    expand = expansion(mult = c(0.2, 0.2))
    ) +
 scale_x_discrete(expand = c(0.2, 0.2))
                            
                             
                             

maia_rainplot <-  maia_rainplot +
  geom_signif(
    comparisons = list(c("HC", "SSD")),
    annotations = "***",          # or "p = 0.004"
    y_position = max(questionnaire_data$MAIA_total, na.rm = TRUE) * 1.05,
    tip_length = 0.01,
    textsize = 10,
    vjust = 0.4,
    color = "black",              # bracket + lines
    textcolor = "black"           # stars/text
  )


ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/maia_rainplot.tiff",
  plot = maia_rainplot,
  device = "tiff",
  dpi = 1200,
  #width = 8.5,   # cm ‚Äì compact but not cramped
  #height = 5.5,  # cm
  #units = "cm",
  compression = "lzw",
  bg = "transparent"
)

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/maia_rainplot.png",
  plot = maia_rainplot,
  device = "png",
  dpi = 1200,
  bg = "transparent"
)




```

```{r publication plot bpq}
library(ggplot2)
library(ggdist)
library(ggsignif)


pal <- c("#7A9E3A", "#D2B100")

names(pal) <- levels(factor(questionnaire_data$group))

bpq_rainplot <- ggplot(
  questionnaire_data,
  aes(x = group, y = BPQ_total, fill = group, color = group)
) +
  stat_halfeye(
    adjust = 0.8,
    width = 0.45,
    #justification = -0.10,
    point_colour = NA,
    alpha = 0.55,              # ‚Üë less transparent
    linewidth = 0.9            # ‚Üë darker density outline
  ) +
  geom_boxplot(
    width = 0.25,
    outlier.shape = NA,
    alpha = 0.35,
    linewidth = 0.7            # ‚Üë box outline strength
  ) +
  geom_point(
    position = position_jitter(width = 0.10, height = 0),
    alpha = 0.75,              # ‚Üë point visibility
    size = 1.4
  ) +
  labs(
    x = "Group",
    y = "BPQ Total"
  ) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "transparent", color = NA),
    plot.background  = element_rect(fill = "transparent", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    panel.grid = element_blank(),
    axis.title = element_text(size = 24),
    axis.text  = element_text(size = 24)
  )+ scale_y_continuous(limits = c(10, NA)#,
   # expand = expansion(mult = c(0.2, 0.2))
    ) +
 scale_x_discrete(expand = c(0.2, 0.2))


ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/bpq_rainplot.tiff",
  plot = bpq_rainplot,
  device = "tiff",
  dpi = 1200,
  #width = 8.5,   # cm ‚Äì compact but not cramped
  #height = 5.5,  # cm
  #units = "cm",
  compression = "lzw",
  bg = "transparent"
)

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/bpq_rainplot.png",
  plot = bpq_rainplot,
  device = "png",
  dpi = 1200,
  bg = "transparent"
)




```

```{r publication plot cds}
library(ggplot2)
library(ggdist)
library(ggsignif)


pal <- c("#7A9E3A", "#D2B100")

names(pal) <- levels(factor(questionnaire_data$group))

cds_rainplot <- ggplot(
  questionnaire_data,
  aes(x = group, y = CDS, fill = group, color = group)
) +
  stat_halfeye(
    adjust = 0.8,
    width = 0.45,
    #justification = -0.10,
    point_colour = NA,
    alpha = 0.55,              # ‚Üë less transparent
    linewidth = 0.9            # ‚Üë darker density outline
  ) +
  geom_boxplot(
    width = 0.25,
    outlier.shape = NA,
    alpha = 0.35,
    linewidth = 0.7            # ‚Üë box outline strength
  ) +
  geom_point(
    position = position_jitter(width = 0.10, height = 0),
    alpha = 0.75,              # ‚Üë point visibility
    size = 1.4
  ) +
  labs(
    x = "Group",
    y = "CDS"
  ) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "transparent", color = NA),
    plot.background  = element_rect(fill = "transparent", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    panel.grid = element_blank(),
    axis.title = element_text(size = 24),
    axis.text  = element_text(size = 24)
  )+ scale_y_continuous(limits = c(10, NA),
    expand = expansion(mult = c(0.2, 0.2))
    ) +
 scale_x_discrete(expand = c(0.2, 0.2))

                            

cds_rainplot <-  cds_rainplot +
  geom_signif(
    comparisons = list(c("HC", "SSD")),
    annotations = "***",          # or "p = 0.004"
    y_position = max(questionnaire_data$CDS, na.rm = TRUE) * 1.05,
    tip_length = 0.01,
    textsize = 10,
    vjust = 0.4,
    color = "black",              # bracket + lines
    textcolor = "black"           # stars/text
  )



ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/cds_rainplot.tiff",
  plot = cds_rainplot,
  device = "tiff",
  dpi = 1200,
  #width = 8.5,   # cm ‚Äì compact but not cramped
  #height = 5.5,  # cm
  #units = "cm",
  compression = "lzw",
  bg = "transparent"
)

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/cds_rainplot.png",
  plot = cds_rainplot,
  device = "png",
  dpi = 1200,
  bg = "transparent"
)




```

#### Exploratory

Radar Plot
```{r}
library(dplyr)
library(fmsb)

# --- Variable sets ---
bpq_vars <- c("BPQ_body_awareness", "BPQ_autonomic", 
              "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic")

maia_vars <- c("MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying",
               "MAIA_attention_regulation", "MAIA_emotional_awareness",
               "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting")

# --- Format names for journal ---
format_names <- function(x) {
  x <- gsub("_", " ", x)
  x <- gsub("bpq", "BPQ", x, ignore.case = TRUE)
  x <- gsub("maia", "MAIA", x, ignore.case = TRUE)
  x <- gsub("\\b(\\w)", "\\U\\1", x, perl = TRUE) # capitalize each word
  return(x)
}

bpq_labels <- format_names(bpq_vars)
maia_labels <- format_names(maia_vars)

# --- Compute means per group ---
bpq_summary <- questionnaire_data %>%
  group_by(group) %>%
  summarise(across(all_of(bpq_vars), ~mean(.x, na.rm = TRUE))) %>%
  as.data.frame()

maia_summary <- questionnaire_data %>%
  group_by(group) %>%
  summarise(across(all_of(maia_vars), ~mean(.x, na.rm = TRUE))) %>%
  as.data.frame()

rownames(bpq_summary) <- bpq_summary$group
bpq_summary <- bpq_summary[, bpq_vars]

rownames(maia_summary) <- maia_summary$group
maia_summary <- maia_summary[, maia_vars]

# --- Helper to build fmsb input ---
make_fmsb_df <- function(df) {
  col_max <- apply(df, 2, max, na.rm = TRUE)
  col_min <- apply(df, 2, min, na.rm = TRUE)
  pad_top <- col_max + 0.1 * abs(col_max)
  pad_bottom <- pmax(col_min - 0.1 * abs(col_min), 0)
  chart_df <- rbind(max = pad_top, min = pad_bottom, df)
  return(as.data.frame(chart_df))
}

bpq_chart <- make_fmsb_df(bpq_summary)
maia_chart <- make_fmsb_df(maia_summary)

# --- Colors ---
col_hc <- "green3"
col_ssd <- "gold"
fill_hc <- adjustcolor(col_hc, alpha.f = 0.35)
fill_ssd <- adjustcolor(col_ssd, alpha.f = 0.35)

# --- Plot BPQ ---
par(mfrow=c(1,1), mar=c(1.5,1.5,2.5,1))
radarchart(
  bpq_chart,
  axistype = 1,
  pcol = c(col_hc, col_ssd),
  pfcol = c(fill_hc, fill_ssd),
  plwd = 2,
  plty = 1,
  cglcol = "grey85",
  cglty = 1,
  axislabcol = "grey30",
  caxislabels = NULL,
  seg = 4,
  pty = NA,
  vlcex = 0.8,
  vlabels = bpq_labels  # journal-friendly names
)
title(main = "BPQ Subscales by Group", line = 0.5)
legend("topright", legend = rownames(bpq_chart)[-c(1,2)], 
       bty = "n", fill = c(fill_hc, fill_ssd), border = c(col_hc, col_ssd), cex=0.9)

# --- Plot MAIA ---
radarchart(
  maia_chart,
  axistype = 1,
  pcol = c(col_hc, col_ssd),
  pfcol = c(fill_hc, fill_ssd),
  plwd = 2,
  plty = 1,
  cglcol = "grey85",
  cglty = 1,
  axislabcol = "grey30",
  caxislabels = NULL,
  seg = 4,
  pty = NA,
  vlcex = 0.75,
  vlabels = maia_labels
)
title(main = "MAIA Subscales by Group", line = 0.5)
legend("topright", legend = rownames(maia_chart)[-c(1,2)], 
       bty = "n", fill = c(fill_hc, fill_ssd), border = c(col_hc, col_ssd), cex=0.9)


```

TEST All BPQ subscales:
```{r lm for loop: BPQs}

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires"

# Initialize a list to store the results
combined_results_bpq <- list()

for (var in BPQ_vars_no_total) {
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  #### LMEM ####
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (std-beta)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate_beta = Std_Coefficient,
    std_beta_95CI_low = CI_low,
    std_beta_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = var) %>%
  select(predictor, outcome, std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)

  # Merge the tables on the predictor variable
  combined_table <- full_join(std_beta_df, coef_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results_bpq[[var]] <- combined_table
  
}

# Combine all results into one dataframe
anova_results_df_bpq_questionnaires <- do.call(rbind, combined_results_bpq)

# get only results of interest
# Filter the results where the predictor "groupSZ"
bpq_results_of_interest <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("groupSSD"), 
]
bpq_results_of_interest_group <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("group"), 
]

# Correct for multiple comparisons using the Benjamini-Hochberg procedure
bpq_results_of_interest$p_adjusted <- p.adjust(bpq_results_of_interest$p_value, method = "BH",  n = length(BPQ_vars_no_total)) #  n = length(BPQ_vars)  

# Format p-values to avoid scientific notation
bpq_results_of_interest$p_adjusted <- format(bpq_results_of_interest$p_adjusted, scientific = FALSE) # ,  digits = 3


# View the results
print(bpq_results_of_interest)

# SAVE 
write.csv(bpq_results_of_interest, file = file.path(plot_path, "lm_results_BPQ-subscales_groupSZ_p-adjusted_2025-09-05.csv"),row.names = FALSE)

```


TEST All MAIA subscales
```{r lm for loop: MAIAs}

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires"

# Initialize a list to store the results
combined_results_maia <- list()

for (var in MAIA_vars_no_total) {
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  #### LMEM ####
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (std-beta)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate_beta = Std_Coefficient,
    std_beta_95CI_low = CI_low,
    std_beta_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = var) %>%
  select(predictor, outcome, std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)

  # Merge the tables on the predictor variable
  combined_table <- full_join(std_beta_df, coef_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results_maia[[var]] <- combined_table
  
}

# Combine all results into one dataframe
anova_results_df_maia_questionnaires <- do.call(rbind, combined_results_maia)

# get only results of interest

# Filter the results where the predictor is either "group" or "groupSZ"
maia_results_of_interest <- anova_results_df_maia_questionnaires[
  anova_results_df_maia_questionnaires$predictor %in% c("groupSSD"), 
]
maia_results_of_interest_group <- anova_results_df_maia_questionnaires[
  anova_results_df_maia_questionnaires$predictor %in% c("group"), 
]
# Correct for multiple comparisons using the Benjamini-Hochberg procedure
maia_results_of_interest$p_adjusted <- p.adjust(maia_results_of_interest$p_value, method = "BH",  n = length(MAIA_vars_no_total)) #  n = length(BPQ_vars)  

print(maia_results_of_interest)

write.csv(maia_results_of_interest, file = file.path(plot_path, "lm_results_MAIA-subscales_group_p-adjusted_2025-09-05.csv"),row.names = FALSE)


```


### 1.2) Interoceptive accuracy is reduced in patients as measured by the Heartbeat Counting Task (HCT). (directional hypothesis)

We will build a regression model for each hypothesis, where the variables of interest (Interoceptive Sensibility and Interoceptive Accuracy data) will be the dependent variable and the diagnostic group (HC vs. SSD) will be the predictor, along with the covariates of age, gender, BMI, and years of education. For the Interoceptive Accuracy model, we will also add Heart Rate (HR), smoking, caffeine intake, knowledge of heart rate, and the Not-Worrying subscale of MAIA as a proxy for anxiety about bodily sensations as covariates.

#### Check for assumptions

Check for Assumptions for lm() (Linear Regression) 1. Linearity The relationship between predictors and the outcome variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The residuals have constant variance across all levels of the predictors. 4. Normality of Residuals The residuals are normally distributed. 5. No Multicollinearity Predictors are not highly correlated with each other. 6. No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

Define the model

```{r}
# compute LM
lm_result <- lm(interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
               data=iacc_data,
               na.action=na.exclude)

```

##### 1. Linearity

```{r}

#plot: shows the relationship between each predictor and the residuals, adjusted for the other predictors in the model.
crPlots(lm_result) 

# you can also plot residuals vs predictions
#If the plot shows random scatter around 0 without any pattern, it suggests linearity. If there's a clear curve, the linearity assumption might be violated.
plot(lm_result$fitted.values, lm_result$residuals)
abline(h = 0, col = "red")

# or same thing but different..plot Residuals vs Fitted
plot(lm_result, which = 1, main = paste("Residuals vs Fitted for IAcc"))



```

##### 2. Independence of Observations 

If the residuals are autocorrelated, this can indicate dependence of observations, hence a violation. You can check residual autocorrelation using the Durbin-Watson test, which tests for autocorrelation in residuals. If the p-value is significant, it suggests that the assumption of independence might be violated.

```{r}

dw_test <- dwtest(lm_result)
print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))

```


##### 3. Homoscedasticity 

If the plot shows a funnel shape (residuals increasing or decreasing as fitted values increase), or if the Breusch-Pagan test gives a significant result, the assumption of homoscedasticity may be violated.

```{r}

# Visual check
plot(lm_result$fitted.values, lm_result$residuals)
abline(h = 0, col = "red")
# test
bptest(lm_result) 

# Scale-Location Plot
plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))


```


##### 4. Normality of Residuals 

If the Q-Q plot shows significant deviation from the line, or if the Shapiro-Wilk test returns a significant result (p-value \< 0.05), the normality assumption may be violated.

```{r}

# Q-Q plot
qqnorm(lm_result$residuals)
qqline(lm_result$residuals, col = "red")
# test
shapiro.test(residuals(lm_result)) # p < 0.05 => Residuals deviate from normality


plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))


```

If so, GLS and transformation of the dependent variable can be used again.

##### 5. No Multicollinearity. 

You can use the Variance Inflation Factor (VIF) to check for multicollinearity.

```{r}
vif_values <- vif(lm_result)
print(paste("VIF for predictors in", var, ":"))
print(vif_values)
```

##### 6. No Outliers or High Leverage Points 

If you see points with Cook‚Äôs distance greater than 1 or leverage values much higher than the average (typically above 2 times the average leverage), they may be outliers or influential points.

```{r}

# Cook's distance
plot(lm_result, which = 4)
# Leverage statistics
hatvalues(lm_result)
plot(hatvalues(lm_result), main = "Leverage Values", ylab = "Leverage", xlab = "Observation Index")
abline(h = threshold, col = "red", lty = 2)  # Add the threshold as a reference line

# Outlier Test
outlierTest(lm_result) # # If the test returns "No Studentized residuals with Bonferroni p < 0.05", it suggests there are no significant outliers.

```


##### 7. Additional diagnostic tests: Model specification test

The RESET test (Regression Specification Error Test) checks whether the model is properly specified. If the model has misspecified relationships (e.g., omitted variables or nonlinear relationships), the RESET test can indicate this. =\> p \< 0.05: Model might be misspecified (e.g., nonlinear relationships, omitted predictors).

```{r}

resettest(lm_result) 

```

If the p-value is less than 0.05, it suggests that the model might be misspecified, meaning the linearity assumption could be violated (nonlinear relationships, omitted variables). To address this, check for non-linear relatinships, check whether more predictors or interactions may be needed in the model, or try alternative/non-linear model types.

#### TEST IAcc

```{r lm}

# 1. Get LM coefficients with CIs (adjusted for covariates)

# Model Summary & Coeffs
summary_lm <- summary(lm_result)
confint_lm <- confint(lm_result)  # 95% CIs for coefficients by default

# Prepare coefficient table
coef_df <- data.frame(
  summary_lm$coefficients,
  confint_lm,
  predictor = rownames(summary_lm$coefficients),
  outcome = "interoceptive_accuracy",
  row.names = NULL
) %>%
  rename(
    estimate = Estimate,
    std_error = Std..Error,
    t_value = t.value,
    p_value = Pr...t..,
    coef_95CI_low = X2.5..,
    coef_95CI_high = X97.5..
  )


# 2. Get standardized (adjusted) betas (effect sizes)
std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate = Std_Coefficient,
    std_95CI_low = CI_low,
    std_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = "interoceptive_accuracy") %>%
  select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)


# 3. Merge raw and standardized results
lm_result_table <- coef_df %>%
  left_join(std_beta_df, by = c("outcome", "predictor")) %>%
  # Reorder columns for clarity
  select(
    predictor, outcome,
    estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
    std_estimate, std_95CI_low, std_95CI_high,           # Standardized
    t_value, p_value
  )

# Filter for your predictor of interest (e.g., "groupSZ")
lm_result_table_SZ <- lm_result_table %>%
  filter(predictor == "groupSSD")  # Replace with your target predictor

```

Hierarchical LM to avoid overfitting
```{r}
# First, let's define the order in which we'll add variables
# Start with your primary predictor (group), then add covariates in order of importance
variable_order <- c("group", "sex", "education_years", "MAIA_not_worrying", "heart_rate_bpm", "smoker", "age", "body_mass_index", 
                     "had_caffeine", "knows_heartrate"
                   )

# Initialize a list to store all models
models <- list()

# Create baseline model (intercept only)
models[["intercept_only"]] <- lm(interoceptive_accuracy ~ 1, 
                                data = iacc_data,
                                na.action = na.omit)

# Build models sequentially
current_formula <- "interoceptive_accuracy ~ 1"
for (var in variable_order) {
  current_formula <- paste(current_formula, "+", var)
  models[[current_formula]] <- lm(as.formula(current_formula), 
                                 data = iacc_data,
                                 na.action = na.omit)
}

# Create a model comparison table
model_comparison <- data.frame(
  Model = names(models),
  R_squared = sapply(models, function(x) summary(x)$r.squared),
  Adj_R_squared = sapply(models, function(x) summary(x)$adj.r.squared),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC),
  stringsAsFactors = FALSE
)

# Calculate change in R-squared at each step
model_comparison$R_squared_change <- c(NA, diff(model_comparison$R_squared))
model_comparison$Adj_R_squared_change <- c(NA, diff(model_comparison$Adj_R_squared))

# View the comparison
print(model_comparison)

# save model 
write.csv(model_comparison, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/iacc_model_comparison.csv", row.names = FALSE)

# You can also examine each model's coefficients individually
# For example, to see the final model:
summary(models[[length(models)]])

# winning model:
best_model <- lm(interoceptive_accuracy ~ group + sex, # + education_years + MAIA_not_worrying +heart_rate_bpm,
                data = iacc_data,
                na.action = na.omit)

# Check assumptions
par(mfrow=c(2,2))
plot(best_model) # Residual diagnostics

# Check multicollinearity
car::vif(best_model) # Values >5 indicate problems

# Check influential points
car::influencePlot(best_model)

#  RE RUN BEST MODEL

# Model Summary & Coeffs
summary_lm <- summary(best_model)
confint_lm <- confint(best_model)  # 95% CIs for coefficients by default

# Prepare coefficient table
coef_df <- data.frame(
  summary_lm$coefficients,
  confint_lm,
  predictor = rownames(summary_lm$coefficients),
  outcome = "interoceptive_accuracy",
  row.names = NULL
) %>%
  rename(
    estimate = Estimate,
    std_error = Std..Error,
    t_value = t.value,
    p_value = Pr...t..,
    coef_95CI_low = X2.5..,
    coef_95CI_high = X97.5..
  )


# 2. Get standardized (adjusted) betas (effect sizes)
std_beta_df <- standardize_parameters(best_model, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate = Std_Coefficient,
    std_95CI_low = CI_low,
    std_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = "interoceptive_accuracy") %>%
  select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)


# 3. Merge raw and standardized results
lm_result_table <- coef_df %>%
  left_join(std_beta_df, by = c("outcome", "predictor")) %>%
  # Reorder columns for clarity
  select(
    predictor, outcome,
    estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
    std_estimate, std_95CI_low, std_95CI_high,           # Standardized
    t_value, p_value
  )

# Filter for your predictor of interest (e.g., "groupSZ")
lm_result_table_SZ <- lm_result_table %>%
  filter(predictor == "groupSSD")  # Replace with your target predictor

# save
write.csv(lm_result_table, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/lm_result_table_best_model_iacc.csv", row.names = FALSE)


```

#### PLOT IAcc

```{r publication plot iacc}
library(ggplot2)
library(ggdist)
library(ggsignif)


pal <- c(HC = "#7A9E3A", SSD = "#D2B100")

names(pal) <- levels(factor(iacc_data$group))

iacc_rainplot <- ggplot(
  iacc_data,
  aes(x = group, y = interoceptive_accuracy, fill = group, color = group)
) +
  stat_halfeye(
    adjust = 0.8,
    width = 0.45,
    #justification = -0.10,
    point_colour = NA,
    alpha = 0.55,              # ‚Üë less transparent
    linewidth = 0.9            # ‚Üë darker density outline
  ) +
  geom_boxplot(
    width = 0.25,
    outlier.shape = NA,
    alpha = 0.35,
    linewidth = 0.7            # ‚Üë box outline strength
  ) +
  geom_point(
    position = position_jitter(width = 0.10, height = 0),
    alpha = 0.75,              # ‚Üë point visibility
    size = 1.4
  ) +
  labs(
    x = "Group",
    y = "Interoceptive Accuracy"
  ) +
  scale_fill_manual(values = pal) +
  scale_color_manual(values = pal) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA),
    plot.background  = element_rect(fill = "transparent", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    panel.grid = element_blank(),
    axis.title = element_text(size = 16),
    axis.text  = element_text(size = 16)
  )+ scale_y_continuous(limits = c(0, NA),
    expand = expansion(mult = c(0.2, 0.12))
    ) +
 scale_x_discrete(expand = c(0.2, 0.2))

                            

iacc_rainplot <-  iacc_rainplot +
  geom_signif(
    comparisons = list(c("HC", "SSD")),
    annotations = "‚Ä¢",          # or "p = 0.004"
    y_position = max(iacc_data$interoceptive_accuracy, na.rm = TRUE) * 1.05,
    tip_length = 0.01,
    textsize = 5,
    vjust = 0.4,
    color = "black",              # bracket + lines
    textcolor = "black"           # stars/text
  )



ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/iacc_rainplot_legend.tiff",
  plot = iacc_rainplot,
  device = "tiff",
  dpi = 1200,
  #width = 8.5,   # cm ‚Äì compact but not cramped
  #height = 5.5,  # cm
  #units = "cm",
  compression = "lzw",
  bg = "transparent"
)

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/iacc_rainplot_legend.png",
  plot = iacc_rainplot,
  device = "png",
  dpi = 1200,
  bg = "transparent"
)




```


### 1.3) At rest, the patients‚Äô Heartbeat Evoked Potential amplitude (HEP; 450-500 ms post-ECG R-peak, in frontal central regions, specifically for the Fp2, F4, F8 electrodes) is different than HC, denoting altered interoception on the neural level. (non-directional hypothesis).

We will run 2√ó3 Analyses of Covariance (ANCOVA) to inspect the HEP differences between the groups (between-subject variable with 2 levels: HC and SSD) across different tasks (within-subject variable with 3 levels: eyes-closed, eyes-open, HCT), while controlling for the covariates: age, gender, body mass index (BMI), educational level, smoking, caffeine intake, HR, QT interval, QTc interval, the R wave amplitude, and Root Mean Square of Successive Differences (RMSSD), and the Not-Worrying subscale of MAIA as a proxy for anxiety about bodily sensations.

Utilizing an ANCOVA will allow us to scrutinize the differential HEP modulation through directed attention between groups (significant interaction effects will be tested with Tukey‚Äôs HSD post-hoc tests). Parallel to Koreki and colleagues (2024), we will focus on the frontal-central regions. We will run an ANCOVA on the HEP amplitude averaged across all frontal-central areas, as well as running individual ANCOVAs for each electrode within this region ('Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 'FC1', 'FC2', 'FC5', 'FC6', 'C3', 'C4', 'Cz'). To address the issue of multiple comparisons and identify assembles of significant electrodes, we will conduct a cluster-based correction analysis.

convert amplitudes from volts to microVolts (not needed for the mean_amplitude col...)
```{r}
# Convert amplitudes from volts to microvolts
roi_hep_data$F4_mean_amplitude <- roi_hep_data$F4_mean_amplitude * 1e6
roi_hep_data$F8_mean_amplitude <- roi_hep_data$F8_mean_amplitude * 1e6
roi_hep_data$Fp2_mean_amplitude <- roi_hep_data$Fp2_mean_amplitude * 1e6


```

Define Task order
```{r}
# Reorder the 'task' variable to appear in the desired order
task_order <- c("eyes-closed", "eyes-open", "hct")
roi_hep_data$task <- factor(roi_hep_data$task, levels = task_order)

```

get channel names
```{r channels, frontal_central_regions}
# I can get any since they are all the same
channels <- roi_hep_data$channels[1]

# Define the character vector
frontal_central_regions <- c('Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 
                              'FC1', 'FC2', 'FC5', 'FC6', 'C3', 'C4', 'Cz')

hypothesis_regions <- c("Fp2", "F4", "F8")
hypothesis_vars <- c("Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude")


```


1)  Three hypothesis-driven Channels

w/ FAMD on covars:
```{r}

# capture all outputs
output_file_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/HEPs/confirmatory_models.txt" 
sink(output_file_path, append = FALSE, split = TRUE) 

roi_hep_data_ec <- roi_hep_data %>%
  filter(task == "eyes-closed")


# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/HEPs"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  pca_vars <- roi_hep_data_ec[, c(
      "heart_rate_bpm", "QT_interval_ms", "age",
      "sex", "smoker", "had_caffeine", "body_mass_index", "education_years", "MAIA_not_worrying","hrv_rmssd_ms", "R_peak_amplitude_mV"
  )]

  # CRITICAL FIX: Explicitly coerce types to eliminate hidden classes 
  
  # Continuous (Numeric) Variables:
  pca_vars$heart_rate_bpm   <- as.numeric(pca_vars$heart_rate_bpm)
  pca_vars$QT_interval_ms   <- as.numeric(pca_vars$QT_interval_ms)
  pca_vars$age              <- as.numeric(pca_vars$age)
  pca_vars$body_mass_index  <- as.numeric(pca_vars$body_mass_index)
  pca_vars$education_years  <- as.numeric(pca_vars$education_years)
  pca_vars$MAIA_not_worrying  <- as.numeric(pca_vars$MAIA_not_worrying)
  pca_vars$hrv_rmssd_ms  <- as.numeric(pca_vars$hrv_rmssd_ms)
  pca_vars$R_peak_amplitude_mV  <- as.numeric(pca_vars$R_peak_amplitude_mV)

  # Categorical (Factor) Variables:
  pca_vars$sex          <- as.factor(pca_vars$sex)
  pca_vars$smoker       <- as.factor(pca_vars$smoker)
  pca_vars$had_caffeine <- as.factor(pca_vars$had_caffeine)

  # Perform FAMD on the robustly typed dataset
  # This should now run without the 'x must be numeric' error.
  pca_result <- FAMD(pca_vars, graph = FALSE) 
  summary(pca_result)
  plot(pca_result, type = "l")  # Scree plot
  roi_hep_data_ec$PC1 <- pca_result$ind$coord[, "Dim.1"] 
  roi_hep_data_ec$PC2 <- pca_result$ind$coord[, "Dim.2"]
  roi_hep_data_ec$PC3 <- pca_result$ind$coord[, "Dim.3"] 

  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + PC1 + PC2 + PC3")), 
                  data = roi_hep_data_ec,
                  na.action = na.omit)

  
  # a. Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # 6. Outliers
  plot(lm_result, which = 4)
  # Outlier Test
  outlierTest(lm_result)
  
  # 7. Additional Diagnostics
  resettest_result <- resettest(lm_result)
  print(paste( "Resettest Result", var, ":", resettest_result))
  print(resettest_result)

  
  # b. Get LM coefficients with CIs (adjusted for covariates)

  # Model Summary & Coeffs
  summary_lm <- summary(lm_result)
  confint_lm <- confint(lm_result)  # 95% CIs for coefficients by default
  
  # Prepare coefficient table
  coef_df <- data.frame(
    summary_lm$coefficients,
    confint_lm,
    predictor = rownames(summary_lm$coefficients),
    outcome = var,
    row.names = NULL
  ) %>%
    rename(
      estimate = Estimate,
      std_error = Std..Error,
      t_value = t.value,
      p_value = Pr...t..,
      coef_95CI_low = X2.5..,
      coef_95CI_high = X97.5..
    )

    
  # c. Get standardized (adjusted) betas (effect sizes)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
    as.data.frame() %>%
    rename(
      std_estimate = Std_Coefficient,
      std_95CI_low = CI_low,
      std_95CI_high = CI_high,
      predictor = Parameter
    ) %>%
    mutate(outcome = var) %>%
    select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)
  
  
  # d. Merge raw and standardized results
  lm_result_table <- coef_df %>%
    left_join(std_beta_df, by = c("outcome", "predictor")) %>%
    # Reorder columns for clarity
    select(
      predictor, outcome,
      estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
      std_estimate, std_95CI_low, std_95CI_high,           # Standardized
      t_value, p_value
    )
  
  # Filter for your predictor of interest (e.g., "groupSZ")
  lm_result_table_SZ <- lm_result_table %>%
    filter(predictor == "groupSSD")  # Replace with your target predictor
  
  # Store the combined results in the list
  combined_results[[var]] <- lm_result_table
  
}

# Combine all results into one dataframe
lm_results_df_heps <- do.call(rbind, combined_results)
# save 
#write.csv(lm_results_df_heps, file = file.path(plot_path, "lm_results_hep_ec.csv"), row.names = FALSE)


# Filter results for group comparison
lm_results_df_heps_groupSZ <- lm_results_df_heps[
  lm_results_df_heps$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
lm_results_df_heps_groupSZ$p_adjusted <- p.adjust(lm_results_df_heps_groupSZ$p_value, method = "BH") 

# View the results
print(lm_results_df_heps_groupSZ)

#stop capture
sink()

#save
#write.csv(lm_results_df_heps_groupSZ, file = file.path(plot_path, "lm_results_hep_ec_lm-groupSZ.csv"), row.names = FALSE)

```

Exploratory all 3 including task!
```{r test all three with lm & check assumptions in a loop}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/HEPs"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index")), 
                  data = roi_hep_data,
                  na.action = na.omit)
  
  # Identify rows used in the linear model
  used_rows <- !is.na(questionnaire_data$group) & 
                complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])
  
  # Create a new dataframe with residuals and corresponding groups
  residuals_data <- data.frame(
    group = questionnaire_data$group[used_rows],
    residuals = residuals(lm_result)
  )
  
  # Step 2: Create a boxplot of residuals by group
  p <- ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Boxplot without separate outlier points
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +  # Show individual data points
    labs(title = paste("Distribution of Residuals by Group for", var), 
         x = "Group", 
         y = "Residuals") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set3")  # Optional: better color scheme
  
  print(p)
  
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Residuals_by_Group_", var, ".png"),
         plot = p,
         width = 8, height = 6, dpi = 300)

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  combined_table <- merge(anova_table, coef_df, by = c("outcome", "predictor"), all = TRUE)
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
}

# Combine all results into one dataframe
anova_results_df_questionnaires_total <- do.call(rbind, combined_results)

# Filter results for group comparison
anova_results_df_questionnaires_total_groupSZ <- anova_results_df_questionnaires_total[
  anova_results_df_questionnaires_total$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_questionnaires_total_groupSZ$p_adjusted <- p.adjust(anova_results_df_questionnaires_total_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_questionnaires_total_groupSZ)

# SAVE !!!
# Save the data frame as a CSV file to the directory
write.csv(
  anova_results_df_questionnaires_total_groupSZ, 
  file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-04-23.csv"),
  row.names = FALSE
)
```


Cluster-Based Correction Analysis:on MNE

### 1.4) Control Analyses

As control analyses, we will compare the demographics and the ECG data of HC and SSD samples with independent samples Student's t-tests for continuous variables (Mann-Whitney U Test in case of assumption violation) and chi-squared test for categorical variables (Fisher's Exact Test if assumptions violated).

ECG

```{r}
# Convert the variable to numeric
roi_hep_data$ecg_mean_amplitude_time_window <- as.numeric(roi_hep_data$ecg_mean_amplitude_time_window)

# convert to mV
roi_hep_data$ecg_mean_amplitude_time_window <- roi_hep_data$ecg_mean_amplitude_time_window * 1e6

# Conduct a one-way ANOVA for ecg_mean_amplitude_time_window across different groups
anova_result <- aov(ecg_mean_amplitude_time_window ~ group, data = roi_hep_data)

# Summary of the ANOVA
summary(anova_result)
capture.output(summary(anova_result), file = paste0(plot_path, "anova_results_ecg.txt"))


# Calculate means for each group
group_means <- tapply(roi_hep_data$ecg_mean_amplitude_time_window, roi_hep_data$group, mean)
write.csv(group_means, paste0(plot_path, "group_means_ecg.csv"))


# Plot the results using boxplots to visualize group differences
ggplot(roi_hep_data, aes(x = group, y = ecg_mean_amplitude_time_window, fill = group)) +
  geom_boxplot() +
  labs(x = "Group", y = "ECG Mean Amplitude (Time Window)") +
  theme_minimal()

ggsave(paste0(plot_path, "ecg_mean_amplitude_by_group_boxplot.png"), width = 8, height = 6, dpi = 300)



```

Basic analysis without covars:
```{r}
# Define dependent variables
dependent_vars <- c("heart_rate_bpm", "hrv_rmssd_ms", "R_peak_amplitude_mV", "QT_interval_ms","QTc_interval_ms", "ecg_mean_amplitude_time_window")

# Initialize a list to store results for both datasets
lm_results <- list()

# Loop through dependent variables
for (dv in dependent_vars) {
  # Model 1: With task (roi_hep_data)
  formula_with_task <- as.formula(paste(dv, "~ group * task"))
  lm_with_task <- lm(formula_with_task, data = roi_hep_data)
  summary_with_task <- summary(lm_with_task)
  
  # Store results for the model with task
  lm_results[[dv]]$with_task <- list(
    formula = formula_with_task,
    summary = summary_with_task,
    coefficients = summary_with_task$coefficients
  )
  
  # Print coefficients for the model with task
  print(paste("Results for", dv, "with task:"))
  print(summary_with_task$coefficients)
  
  # Model 2: Without task (roi_hep_data_ec)
  formula_without_task <- as.formula(paste(dv, "~ group"))
  lm_without_task <- lm(formula_without_task, data = roi_hep_data_ec)
  summary_without_task <- summary(lm_without_task)
  
  # Store results for the model without task
  lm_results[[dv]]$without_task <- list(
    formula = formula_without_task,
    summary = summary_without_task,
    coefficients = summary_without_task$coefficients
  )
  
  # Print coefficients for the model without task
  print(paste("Results for", dv, "without task:"))
  print(summary_without_task$coefficients)
}

# Access stored results for a specific variable (e.g., Fp2_mean_amplitude with task)
print(lm_results[["Fp2_mean_amplitude"]]$with_task$coefficients)

# Access stored results for the same variable without task
print(lm_results[["Fp2_mean_amplitude"]]$without_task$coefficients)

```
Same with std betas
```{r}
library(dplyr)
library(parameters)  # for standardize_parameters()

# Make sure task is a factor
roi_hep_data$task <- as.factor(roi_hep_data$task)

ecg_task_results_list <- list()

for (t in levels(roi_hep_data$task)) {
  
  # Subset for this task
  dat_t <- roi_hep_data %>% filter(task == t)
  
  # Model: group effect within this task
  lm_t <- lm(ecg_mean_amplitude_time_window ~ group, data = dat_t)
  sum_t <- summary(lm_t)
  
  # Raw coefficients
  coef_df <- as.data.frame(sum_t$coefficients)
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- "ecg_mean_amplitude_time_window"
  coef_df$task <- t
  colnames(coef_df)[1:4] <- c("estimate_b", "std_error", "t_value", "p_value")
  
  # Standardized betas
  std_df <- standardize_parameters(lm_t, method = "refit") %>%
    as.data.frame() %>%
    rename(
      std_estimate_beta = Std_Coefficient,
      std_beta_95CI_low = CI_low,
      std_beta_95CI_high = CI_high,
      predictor = Parameter
    ) %>%
    mutate(
      outcome = "ecg_mean_amplitude_time_window",
      task = t
    ) %>%
    select(predictor, outcome, task,
           std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)
  
  # Merge raw + standardized
  merged_t <- left_join(coef_df, std_df,
                        by = c("predictor", "outcome", "task"))
  
  ecg_task_results_list[[t]] <- merged_t
}

# Stack all tasks into one table
ecg_task_table <- bind_rows(ecg_task_results_list)

# üîé Extract ONLY the group effect (SSD vs HC) per task
ecg_task_groupSSD <- ecg_task_table %>%
  filter(predictor == "groupSSD")

ecg_task_groupSSD

write.csv(
  ecg_task_groupSSD,
  "/Users/denizyilmaz/Desktop/BrainTrain/Results/control_analysis/ecg_mean_group_by_task_with_std_beta.csv",
  row.names = FALSE
)



```

## 2) Interoception measures will relate to global clinical outcomes (symptom severity measured by PANSS Total and cognitive deficits measured by BACS composite global scores) in patients with SSD.

The link between interoception measures (Interoceptive Sensibility, Interoceptive Accuracy, HEP amplitude) and global clinical outcomes (symptom severity measured by PANSS Total and cognitive deficits measured by BACS composite global scores) in patients with SSD, will be assessed by partial correlation analyses, controlling for age, gender, BMI, years of education, chlorpromazine equivalent dose, illness duration. For HCT and HEP analyses, we will also add smoking, caffeine intake, and clozapine dose as covariates.


```{r fun w CI & PCA}
library(dplyr)
library(ppcor)
library(FactoMineR)
library(tidyr)
library(tibble)

perform_partial_correlation_FAMD <- function(data, x_col, y_col, control_vars, n_pcs = 3) {
  
  # --- Subset relevant columns + drop missing rows ---
  subset_vars <- unique(c(x_col, y_col, control_vars))
  partial_corr_data <- data %>%
    select(all_of(subset_vars)) %>%
    na.omit()
  
  n <- nrow(partial_corr_data)
  k <- n_pcs  # number of PCs used as covariates
  
  if (n == 0) stop("No complete cases available after na.omit().")
  
  # --- Explicitly coerce numeric and factor variables ---
  numeric_vars <- sapply(partial_corr_data[, control_vars], is.numeric)
  cat_vars <- !numeric_vars
  
  for (v in names(numeric_vars[numeric_vars])) {
    partial_corr_data[[v]] <- as.numeric(partial_corr_data[[v]])
  }
  for (v in names(cat_vars[cat_vars])) {
    partial_corr_data[[v]] <- as.factor(partial_corr_data[[v]])
  }
  
  # --- FAMD on control variables ---
  famd_res <- FAMD(partial_corr_data[, control_vars], graph = FALSE)
  
  # Add first n_pcs PCs to data
  for (i in 1:n_pcs) {
    pc_name <- paste0("PC", i)
    partial_corr_data[[pc_name]] <- famd_res$ind$coord[, paste0("Dim.", i)]
  }
  
  pc_names <- paste0("PC", 1:n_pcs)
  
  # --- Run partial correlation using PCs ---
  x_vec <- as.numeric(partial_corr_data[[x_col]])
  y_vec <- as.numeric(partial_corr_data[[y_col]])
  z_df  <- partial_corr_data[, pc_names]
  
  res <- pcor.test(x_vec, y_vec, z_df)
  r <- as.numeric(res$estimate)
  
  # --- Compute 95% CI using Fisher z ---
  if ((n - k - 3) > 0) {
    se <- 1 / sqrt(n - k - 3)
    z_val <- 0.5 * log((1 + r) / (1 - r))
    z_low <- z_val - qnorm(0.975) * se
    z_high <- z_val + qnorm(0.975) * se
    ci_lower <- (exp(2 * z_low) - 1) / (exp(2 * z_low) + 1)
    ci_upper <- (exp(2 * z_high) - 1) / (exp(2 * z_high) + 1)
  } else {
    ci_lower <- NA_real_
    ci_upper <- NA_real_
  }
  
  # --- Return tidy result ---
  result_df <- tibble(
    variable_x = x_col,
    variable_y = y_col,
    estimate = r,
    p_value = as.numeric(res$p.value),
    t_statistic = as.numeric(res$statistic),
    df = as.numeric(res$gp),
    ci_lower = ci_lower,
    ci_upper = ci_upper,
    n = n,
    n_control_vars = k
  )
  
  return(result_df)
}



```

partial cor fun with task option
```{r}
perform_partial_correlation <- function(data, columns_to_keep, x_col, y_col, control_vars, task_filter_col = NULL, task_filter_value = NULL) {
  library(dplyr)
  library(ppcor)  # For partial correlation
  
  # Subset the data to include only the specified columns and apply the task filter if provided
  partial_corr_data <- data %>%
    filter(if (!is.null(task_filter_col) && !is.null(task_filter_value)) 
             !!sym(task_filter_col) == task_filter_value else TRUE) %>%
    select(all_of(columns_to_keep)) %>%
    na.omit()  # Remove rows with missing values
  
  # Convert necessary columns to numeric for partial correlation
  partial_corr_data <- partial_corr_data %>%
    mutate(across(
      all_of(c("age.x", "sex.x", "education_years.x", "body_mass_index.x", x_col, y_col)),
      as.numeric
    )) %>%
    mutate(sex = as.numeric(as.factor(sex.x)))  # Convert to numeric if `sex.x` is categorical
  
  # Perform the partial correlation
  partial_corr_result <- pcor.test(
    x = partial_corr_data[[x_col]],
    y = partial_corr_data[[y_col]],
    z = partial_corr_data %>% select(all_of(control_vars))
  )
  
  return(partial_corr_result)
}

```

###  Self reported Interoception & PANSS Total

TEST: interoceptive sensibility and PANSS total
```{r using the function maia}
columns_partial_corr <-  c("panss_total", "MAIA_total","age", "sex", "education_years", "body_mass_index", "cpz")
control_vars <-  c("age", "sex", "education_years", "body_mass_index", "cpz")

partial_corr_result_maia_panss <- perform_partial_correlation(corr_questionnaire_clinical_data, columns_partial_corr, "panss_total", "MAIA_total",  control_vars)


# Save the data frame to CSV
write.csv(partial_corr_result_maia_panss, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/partial_corr_results_panss_maia.csv", row.names = FALSE)


```

```{r using the function bpq}
columns_partial_corr <-  c("panss_total", "BPQ_total","age", "sex", "education_years", "body_mass_index", "cpz")
control_vars <-  c("age", "sex", "education_years", "body_mass_index", "cpz")

partial_corr_result_bpq_panss <- perform_partial_correlation(corr_questionnaire_clinical_data, columns_partial_corr, "panss_total", "BPQ_total",  control_vars)

# Save the data frame to CSV
write.csv(partial_corr_result_bpq_panss, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/partial_corr_results_panss_bpq.csv", row.names = FALSE)

```

```{r using the function cds}
columns_partial_corr <-  c("panss_total", "CDS","age", "sex", "education_years", "body_mass_index", "cpz")
control_vars <-  c("age", "sex", "education_years", "body_mass_index", "cpz")

partial_corr_result_cds_panss <- perform_partial_correlation(corr_questionnaire_clinical_data, columns_partial_corr, "panss_total", "CDS",  control_vars)

# Save the data frame to CSV
write.csv(partial_corr_result_cds_panss, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/partial_corr_results_panss_cds.csv", row.names = FALSE)

```


### Interoceptive Accuracy & PANSS Total

add heart_rate_bpm as a covar
```{r}
corr_questionnaire_clinical_data

# Subset the data to include only the necessary columns
hr_data <- roi_hep_clinical_data %>%
  filter(task == "hct")  # Filter for the specific task

# Merge heart_rate_bpm from hr_data with corr_questionnaire_clinical_data
corr_questionnaire_clinical_data_hr <- corr_questionnaire_clinical_data %>%
  left_join(
    hr_data %>% select(subject, heart_rate_bpm),  # Select only the key and desired column
    by = "subject"  # Specify the key for merging
  )



```

TEST: interoceptive accuracy and PANSS total
```{r using the function}

columns_partial_corr <-  c("panss_total", "interoceptive_accuracy", "age", "sex", "education_years", "body_mass_index", "cpz") #  "heart_rate_bpm",
control_vars <-  c("age", "sex", "education_years", "body_mass_index",  "cpz") #"heart_rate_bpm",

partial_corr_result_iacc_panss <- perform_partial_correlation(corr_iacc_clinical_data, columns_partial_corr, "panss_total", "interoceptive_accuracy",  control_vars)


# Save the data frame to CSV
write.csv(partial_corr_result_iacc_panss, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/partial_corr_result_iacc_panss_no_hr.csv", row.names = FALSE)

```

PLOT: interoceptive acuracy and PANSS total
```{r panss and iacc manuscript}
data_partial_corr_clean <- corr_iacc_clinical_data %>%
  select(interoceptive_accuracy, panss_total, age, sex, education_years,
         body_mass_index, cpz, heart_rate_bpm) %>%
  na.omit()

fit_y <- lm(interoceptive_accuracy ~ age + sex + education_years + body_mass_index + cpz + heart_rate_bpm,
            data = data_partial_corr_clean)

fit_x <- lm(panss_total ~ age + sex + education_years + body_mass_index + cpz + heart_rate_bpm,
            data = data_partial_corr_clean)

residuals_df <- data.frame(
  residuals_x = residuals(fit_x),
  residuals_y = residuals(fit_y)
)

# partial correlation (via residuals)
ct <- cor.test(residuals_df$residuals_x, residuals_df$residuals_y)

iacc_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 2.2, alpha = 0.75, color = "black") +
  geom_smooth(method = "lm", se = TRUE, color = "black", linewidth = 0.8) +
  labs(
    x = "Adjusted PANSS Total",
    y = "Adjusted Interoceptive Accuracy"
  ) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.06))) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    plot.title = element_blank(),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    axis.title = element_text(size = 16),
    axis.text  = element_text(size = 16)
  ) 

iacc_panss_plot

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/iacc_panss_partialcorr.png",
  plot = iacc_panss_plot,
  device = "png",
  dpi = 1200)
```

###  HEPs & PANSS Total

TEST: PANSS Total and Fp2 EC
```{r}
corr_clinical_hep_roi_data <- corr_clinical_hep_roi_data %>%
  mutate(
    Fp2_mean_amplitude = Fp2_mean_amplitude * 1e6,
    F4_mean_amplitude  = F4_mean_amplitude  * 1e6,
    F8_mean_amplitude  = F8_mean_amplitude  * 1e6
  )

# Specify columns to keep
columns_partial_corr <- c("Fp2_mean_amplitude", "panss_total","age", "sex", "education_years", "body_mass_index", "cpz")

# Subset the data to include only the necessary columns
partial_corr_data <- corr_clinical_hep_roi_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    panss_total = as.numeric(panss_total),
    Fp2_mean_amplitude = as.numeric(Fp2_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$Fp2_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "Fp2_mean_amplitude","age", "sex", "education_years", "body_mass_index") # "heart_rate_bpm"

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)

```

PLOT: PANSS Total and Fp2 EC
```{r fp2 panss for manuscript}

fit_y <- lm(Fp2_mean_amplitude ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

fit_x <- lm(panss_total ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

residuals_df <- data.frame(
  residuals_x = residuals(fit_x),
  residuals_y = residuals(fit_y)
)
# partial correlation (via residuals)
ct <- cor.test(residuals_df$residuals_x, residuals_df$residuals_y)

Fp2_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 2.2, alpha = 0.75, color = "black") +
  geom_smooth(method = "lm", se = TRUE, color = "black", linewidth = 0.8) +
  labs(
    x = "Adjusted PANSS Total",
    y = "Adj. Fp2 Mean Amplitude (EC)"
  ) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.06))) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    plot.title = element_blank(),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    axis.title = element_text(size = 24),
    axis.text  = element_text(size = 24)
  ) 

Fp2_ec_panss_plot

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/fp2_ec_panss_partialcorr.png",
  plot = Fp2_ec_panss_plot,
  device = "png",
  dpi = 1200)
```

TEST: PANSS Total and F4 EC
```{r}

# Specify columns to keep
columns_partial_corr <- c("F4_mean_amplitude", "panss_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    panss_total = as.numeric(panss_total),
    F4_mean_amplitude = as.numeric(F4_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$panss_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "F4_mean_amplitude","age", "sex", "education_years", "body_mass_index") #  "heart_rate_bpm"

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F4 EC
```{r f4 panss for manuscript}

fit_y <- lm(F4_mean_amplitude ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

fit_x <- lm(panss_total ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

residuals_df <- data.frame(
  residuals_x = residuals(fit_x),
  residuals_y = residuals(fit_y)
)
# partial correlation (via residuals)
ct <- cor.test(residuals_df$residuals_x, residuals_df$residuals_y)

F4_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 2.2, alpha = 0.75, color = "black") +
  geom_smooth(method = "lm", se = TRUE, color = "black", linewidth = 0.8) +
  labs(
    x = "Adjusted PANSS Total",
    y = "Adj. F4 Mean Amplitude (EC)"
  ) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.06))) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    plot.title = element_blank(),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    axis.title = element_text(size = 22),
    axis.text  = element_text(size = 22)
  ) 

F4_ec_panss_plot

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/f4_ec_panss_partialcorr.png",
  plot = F4_ec_panss_plot,
  device = "png",
  dpi = 1200)
```

TEST: PANSS Total and F8 EC
```{r}

# Specify columns to keep
columns_partial_corr <- c("F8_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(F8_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "F8_mean_amplitude","age", "sex", "education_years", "body_mass_index") # , "heart_rate_bpm" 

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F8 EC
```{r f4 panss for manuscript}

fit_y <- lm(F8_mean_amplitude ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

fit_x <- lm(panss_total ~ age + sex + education_years + body_mass_index,
            data = partial_corr_data)

residuals_df <- data.frame(
  residuals_x = residuals(fit_x),
  residuals_y = residuals(fit_y)
)
# partial correlation (via residuals)
ct <- cor.test(residuals_df$residuals_x, residuals_df$residuals_y)

F8_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 2.2, alpha = 0.75, color = "black") +
  geom_smooth(method = "lm", se = TRUE, color = "black", linewidth = 0.8) +
  labs(
    x = "Adjusted PANSS Total",
    y = "Adj. F8 Mean Amplitude (EC)"
  ) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.06))) +
  theme_classic(base_family = "Arial", base_size = 30) +
  theme(
    plot.title = element_blank(),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    axis.line = element_line(color = "black", linewidth = 0.5),
    axis.title = element_text(size = 22),
    axis.text  = element_text(size = 22)
  ) 

F8_ec_panss_plot

ggsave(
  filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/manuscript_plots/f8_ec_panss_partialcorr.png",
  plot = F8_ec_panss_plot,
  device = "png",
  dpi = 1200)
```

# ECG Control Analysis !!!

Compare the Evoked (Mean) ECGs that I masked for the HEP time points, to show no differences in the HEP window For this use the ecg_outputs csv s and you can find the mean amplitudes of ECG there.

###### EXPLORATORY:  Partial Least Squares

Residualized
```{r}


# Columns to residualize (all interoceptive / HEP / HR / HRV / questionnaires)
intero_vars <- c(
  "interoceptive_accuracy",
  "BPQ_body_awareness","BPQ_supra_diaphragmatic","BPQ_sub_diaphragmatic",
  "CDS",
  "MAIA_noticing","MAIA_body_listening","MAIA_emotional_awareness",
  "MAIA_not_distracting","MAIA_not_worrying","MAIA_attention_regulation","MAIA_self_regulation","MAIA_trusting",
  "hep_pls_all_tasks","heart_rate_bpm_all_tasks","hrv_rmssd_ms_all_tasks"
)

cli_vars <- c("panss_pos", "panss_neg", "panss_gen")

# Nuisance formula
nuisance_formula <- ~ age + sex + body_mass_index + education_years + cpz

# Function to residualize one column
resid_var <- function(var_name, df) {
  lm_formula <- as.formula(paste(var_name, "~ age + sex + body_mass_index + education_years + cpz"))
  resid(df[[var_name]] - predict(lm(lm_formula, data = df)))
}

# Apply to all variables and bind as new columns with suffix "_resid"
pls_data <- pls_data %>%
  mutate(across(all_of(intero_vars),
                ~ resid(lm(.x ~ age + sex + body_mass_index + education_years + cpz, data = pls_data)),
                .names = "{.col}_resid"))


```

```{r define PLS blocks & run PLS}

#library(mixOmics)

# Interoceptive block: residualized variables
intero_block <- pls_data %>%
  select(ends_with("_resid")) %>%
  as.data.frame()

# Clinical block: raw PANSS
clinical_block <- pls_data %>%
  select(panss_pos, panss_neg, panss_gen) %>%
  as.data.frame()

# -------------------------------------------------
# Optional: scale the blocks (center & scale) --------------------
# pls() will centre by default; scaling improves comparability
intero_block  <- scale(intero_block)
clinical_block <- scale(clinical_block)

```

PLSR
```{r}

# -------------------------------------------------
# 1. Fit the PLS Model with Jackknifing
# -------------------------------------------------

# Y Block (Response): clinical_block
# X Block (Predictors): intero_block
# Note: Since blocks are already scaled/centered manually,
# we set scale = FALSE and center = FALSE.

# Determine the maximum number of components possible based on the size of X
# If you prefer a smaller, fixed number, adjust ncomp accordingly (e.g., ncomp = 5)
#max_ncomp <- min(nrow(intero_block) - 1, ncol(intero_block)) 
max_ncomp = 2

pls_model <- plsr(clinical_block ~ intero_block, 
                  ncomp = max_ncomp, 
                  # Crucial for p-values: enable cross-validation (CV) and jackknife
                  validation = "CV", 
                  jackknife = TRUE,
                  # Blocks already prepared externally
                  scale = TRUE,
                  center = TRUE)

# -------------------------------------------------
# 2. Extract Loadings
# -------------------------------------------------

# The 'loadings' function extracts the X Loadings (p) [4, 5]
X_loadings <- loadings(pls_model)
print("X Loadings (Loadings for Predictor Block - Interoceptive Variables):")
print(X_loadings)

# The 'Yloadings' function extracts the Y Loadings (q) [4]
Y_loadings <- Yloadings(pls_model)
print("Y Loadings (Loadings for Response Block - Clinical Variables):")
print(Y_loadings)

# -------------------------------------------------
# 3. Obtain P-values via Jackknife T-test
# -------------------------------------------------

# The jack.test performs approximate t-tests on the regression coefficients (B) [3, 6].
# Specify the number of components you wish to test (e.g., ncomp = 3)
# Replace 'N_components_of_interest' with your desired number of components 
N_components_of_interest <- 3 

pls_jack_test <- jack.test(pls_model, ncomp = 2)#N_components_of_interest)

# Print the full results, including coefficients, t-values, and p-values [6, 7]
print("Jackknife Test Results (P-values for Regression Coefficients):")
print(pls_jack_test)

# You can access the p-value array directly:
p_values_B <- pls_jack_test$pvalues
print("Extracted P-values:")
print(p_values_B)


# -------------------------------------------------
# 4. Component-level permutation test
# -------------------------------------------------
n_perm <- 1000
comp_pvals <- numeric(max_ncomp)  # store p-values per component

# Compute component-level significance using correlation between scores
comp_pvals <- numeric(max_ncomp)

X_scores <- scores(pls_model, type="X")
Y_scores <- scores(pls_model, type="Y")

for(comp in 1:max_ncomp){
  obs_cor <- cor(X_scores[, comp], Y_scores[, comp])
  perm_cor <- numeric(n_perm)
  
  for(i in 1:n_perm){
    perm_y <- clinical_block[sample(nrow(clinical_block)), ]
    perm_model <- plsr(perm_y ~ intero_block, ncomp=max_ncomp, scale=FALSE, center=FALSE)
    perm_Xs <- scores(perm_model, type="X")
    perm_Ys <- scores(perm_model, type="Y")
    perm_cor[i] <- cor(perm_Xs[, comp], perm_Ys[, comp])
  }
  
  comp_pvals[comp] <- mean(abs(perm_cor) >= abs(obs_cor))  # two-sided test
}

names(comp_pvals) <- paste0("Comp", 1:max_ncomp)
print(comp_pvals)

### REPLACE 4 with the following corrected code:

set.seed(123)
n_perm <- 1000
comp_pvals <- numeric(max_ncomp)  # store p-values per component
 
X_scores <- scores(pls_model, type="X")
Y_scores <- scores(pls_model, type="Y")
 
for(comp in 1:max_ncomp){
     obs_cor <- cor(X_scores[, comp], Y_scores[, comp])
     perm_cor <- numeric(n_perm)
     
     for(i in 1:n_perm){
         perm_y <- clinical_block[sample(nrow(clinical_block)), ]
         perm_model <- plsr(perm_y ~ intero_block, ncomp=max_ncomp, scale=FALSE, center=FALSE)
         perm_Xs <- scores(perm_model, type="X")
         perm_Ys <- scores(perm_model, type="Y")
         perm_cor[i] <- cor(perm_Xs[, comp], perm_Ys[, comp])
     }
     
     comp_pvals[comp] <- mean(abs(perm_cor) >= abs(obs_cor))
 }
 
 names(comp_pvals) <- paste0("Comp", 1:max_ncomp)
 print(comp_pvals)




```

Save results
```{r}
# --------------------------
# Create Results Folder
# --------------------------
res_folder <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/PLS"
dir.create(res_folder, showWarnings = FALSE, recursive = TRUE)

# Save X loadings
write.csv(X_loadings, file = file.path(res_folder, "PLS_X_loadings.csv"), row.names = TRUE)

# Save Y loadings
write.csv(Y_loadings, file = file.path(res_folder, "PLS_Y_loadings.csv"), row.names = TRUE)

# Save jackknife p-values
# Convert 3D array to long format
library(tidyr)
library(dplyr)

pval_long <- as.data.frame(p_values_B[,,1]) %>% 
  mutate(var = rownames(.)) %>% 
  pivot_longer(cols = -var, names_to = "response", values_to = "pvalue")

write.csv(pval_long, file = file.path(res_folder, "PLS_jackknife_pvalues.csv"), row.names = FALSE)

# Save component-level permutation p-values
comp_df <- data.frame(
  Component = names(comp_pvals),
  pvalue = comp_pvals
)
write.csv(comp_df, file = file.path(res_folder, "PLS_component_pvalues.csv"), row.names = FALSE)

```

PLOT:
```{r}
# Extract X and Y loadings
load_x <- as.data.frame(loadings(pls_model))
load_x$var <- rownames(load_x)
load_x$block <- "Interoceptive/Physio"

load_y <- as.data.frame(Yloadings(pls_model))
load_y$var <- rownames(load_y)
load_y$block <- "Clinical"

# Pivot longer for ggplot
library(tidyr)
library(dplyr)
library(ggplot2)

# Example if you have 2 components
load_x <- data.frame(
  Comp1 = pls_res$loadings$X[,1],
  Comp2 = pls_res$loadings$X[,2],
  var = rownames(pls_res$loadings$X),
  block = "Interoceptive/Physio"
)

load_y <- data.frame(
  Comp1 = pls_res$loadings$Y[,1],
  Comp2 = pls_res$loadings$Y[,2],
  var = rownames(pls_res$loadings$Y),
  block = "Clinical"
)

# Now pivot_longer will work
load_x_long <- load_x %>% pivot_longer(cols = starts_with("Comp"), names_to = "component", values_to = "loading")
load_y_long <- load_y %>% pivot_longer(cols = starts_with("Comp"), names_to = "component", values_to = "loading")

load_df <- bind_rows(load_x_long, load_y_long)


# Plot
p <- ggplot(load_df, aes(x = reorder(var, loading), y = loading, fill = block)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~component, scales = "free_y") +
  labs(
    x = "",
    y = "PLS Loading",
    title = "PLS Components 1 & 2: X (Interoceptive/Physio) and Y (Clinical) Loadings"
  ) +
  scale_fill_manual(values = c("skyblue", "salmon")) +
  theme_minimal(base_size = 13)

# Save
#ggsave(filename = "/Users/denizyilmaz/Desktop/BrainTrain/Results/PLS/PLS_loadings_comp1_2_plsr.png", plot = p, width = 12, height = 8, dpi = 300)

```


